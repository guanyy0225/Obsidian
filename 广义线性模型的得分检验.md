
好的，这是一个非常深入且核心的统计学问题。我们来详细拆解广义线性模型（GLM）的得分检验理论，以及广义残差 `r` 和权重矩阵 `W` 的数学原理。

### 1. 广义线性模型 (GLM) 的得分检验理论

得分检验（Score Test）是一种在**零假设 ($H_0$)** 成立的条件下构建的检验方法。它的核心思想是：**如果零假设是正确的，那么在零假设参数下的最大似然估计处，完整模型的对数似然函数关于“备择假设中新增参数”的梯度（斜率）应该接近于零。**

#### **数学设定**

假设我们的完整模型（Full Model）参数为 $\theta = (\beta, \gamma)$，其中 $\beta$ 是我们感兴趣的参数（例如，新基因型的效应），而 $\gamma$ 是零模型中已有的参数（例如，截距和协变量的效应）。

我们要检验的原假设是 $H_0: \beta = \beta_0$ （通常是 $\beta_0=0$）。

1.  **对数似然函数**: 记为 $\ell(\theta) = \ell(\beta, \gamma)$。
2.  **得分函数 (Score Function)**: 是对数似然函数关于所有参数的**一阶偏导数（梯度）**向量：
    $$
    U(\theta) = \frac{\partial \ell(\theta)}{\partial \theta} = \begin{pmatrix} U_\beta(\theta) \\ U_\gamma(\theta) \end{pmatrix}
    $$
3.  **信息矩阵 (Information Matrix)**: 是得分函数方差-协方差矩阵，等于负的对数似然函数**二阶偏导数（Hessian矩阵）**的期望：
    $$
    I(\theta) = -E\left[\frac{\partial^2 \ell(\theta)}{\partial \theta \partial \theta^T}\right] = \begin{pmatrix} I_{\beta\beta} & I_{\beta\gamma} \\ I_{\gamma\beta} & I_{\gamma\gamma} \end{pmatrix}
    $$

#### **得分检验的推导**

1.  **只拟合零模型**: 在 $H_0: \beta = 0$ 的条件下，我们最大化 $\ell(0, \gamma)$，得到 $\gamma$ 的估计值，记为 $\tilde{\gamma}$。
2.  **评估得分**: 我们在点 $\tilde{\theta} = (0, \tilde{\gamma})$ 处评估**完整模型**的得分函数 $U(\tilde{\theta})$。
    *   根据最大似然估计的性质，由于 $\tilde{\gamma}$ 是零模型的最优解，所以 $U_\gamma(0, \tilde{\gamma}) = 0$。
    *   因此，得分向量在 $\tilde{\theta}$ 处变为：
        $$
        U(\tilde{\theta}) = \begin{pmatrix} U_\beta(0, \tilde{\gamma}) \\ 0 \end{pmatrix}
        $$
        我们只需要关心 $U_\beta(0, \tilde{\gamma})$，这部分通常简记为 $U_\beta$。
3.  **得分统计量**: 在 $H_0$ 下，$U(\tilde{\theta})$ 近似服从均值为0，协方差矩阵为 $I(\tilde{\theta})$ 的正态分布。得分检验统计量 $S$ 是其二次型：
    $$
    S = U(\tilde{\theta})^T [I(\tilde{\theta})]^{-1} U(\tilde{\theta})
    $$
    利用分块矩阵求逆公式，可以证明这个统计量等价于：
    $$
    S = U_\beta^T (I_{\beta\beta} - I_{\beta\gamma}I_{\gamma\gamma}^{-1}I_{\gamma\beta})^{-1} U_\beta
    $$
    其中所有信息矩阵块都在 $\tilde{\theta}$ 处评估。在 $H_0$ 下，$S$ 近似服从自由度为 $\text{dim}(\beta)$ 的卡方分布。

### 2. 广义残差 (r) 和权重矩阵 (W) 的数学原理

现在，我们来看对于GLM，上面那些抽象的 $U_\beta$ 和信息矩阵块具体是什么。这正是广义残差 `r` 和权重矩阵 `W` 的由来。

#### **GLM 的基本设定**

对于一个GLM，其对数似然函数可以写为：
$$
\ell = \sum_{i=1}^n \ell_i = \sum_{i=1}^n \log f(y_i; \mu_i)
$$
其中均值 $\mu_i$ 通过链接函数 $g(\mu_i) = \eta_i = Z_i^T \gamma + G_i^T \beta$ 与预测变量关联。

#### **得分函数的推导 (链式法则)**

我们来求对数似然关于参数 $\beta$ 的导数（即得分 $U_\beta$）：
$$
U_\beta = \frac{\partial \ell}{\partial \beta} = \sum_{i=1}^n \frac{\partial \ell_i}{\partial \beta} = \sum_{i=1}^n \frac{\partial \ell_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial \beta}
$$
*   **第一部分**: $\frac{\partial \ell_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}$ （这是GLM的一个基本性质）。
*   **第二部分**: $\frac{\partial \mu_i}{\partial \eta_i}$ 是均值对线性预测器的导数。
*   **第三部分**: $\frac{\partial \eta_i}{\partial \beta} = G_i$ （待检验变量的设计矩阵行）。

将它们组合起来：
$$
U_\beta = \sum_{i=1}^n \frac{y_i - \mu_i}{Var(y_i)} \frac{\partial \mu_i}{\partial \eta_i} G_i
$$

#### **定义广义残差 `r` 和权重 `W`**

现在，我们把上面的公式在**零模型 ($H_0$)** 的估计值 $\tilde{\mu}_i$ 处进行评估。在零模型下，$g(\tilde{\mu}_i) = Z_i^T \tilde{\gamma}$。

我们定义两个量：

1.  **权重 (Weight) $W_i$**:
    $$
    W_i = \left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2 \frac{1}{Var(y_i)}
    $$

2.  **广义残差 (Generalized Residual) 或 工作因变量 (Working Variate) $r_i$**:
    $$
    r_i = (y_i - \mu_i) \frac{\partial \eta_i}{\partial \mu_i}
    $$

将 $U_\beta$ 的表达式重写，我们可以发现：
$$
U_\beta = \sum_{i=1}^n G_i \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right) = \sum_{i=1}^n G_i \frac{W_i}{d\mu_i/d\eta_i} \frac{y_i - \mu_i}{Var(y_i)} \times \dots
$$
经过一些代数变换，可以得到一个更简洁的形式。特别地，得分 $U_\beta$ 可以被写成：
$$
U_\beta = \sum_{i=1}^n G_i (y_i^* - \mu_i^*) = G^T r
$$
这里的 $r$ 就是在零模型下计算出的一个特定形式的残差向量，它结合了原始残差 $(y_i - \mu_i)$ 和链接函数的影响。**您代码中计算的 `residuals` 正是这个广义残差 `r` 的一个具体实现**。

#### **信息矩阵与权重 `W`**

同样，可以证明GLM的信息矩阵 $I$ 的对角块 $I_{\gamma\gamma}$ 和 $I_{\beta\beta}$ 可以被近似为：
$$
I_{\gamma\gamma} \approx Z^T W Z
$$
$$
I_{\beta\beta} \approx G^T W G
$$
$$
I_{\beta\gamma} \approx G^T W Z
$$
其中 $W$ 是一个对角矩阵，其对角线元素为 $W_i$。

**这正是您代码中 `weights` 的数学意义**。它就是信息矩阵的“核心”。

#### **将所有部分组合起来**

现在，我们将得分统计量的分块矩阵公式与GLM的组件结合：
*   $U_\beta = G^T r$
*   $I_{\beta\beta} = G^T W G$
*   $I_{\beta\gamma} = G^T W Z$
*   $I_{\gamma\gamma} = Z^T W Z$

代入 $S = U_\beta^T (I_{\beta\beta} - I_{\beta\gamma}I_{\gamma\gamma}^{-1}I_{\gamma\beta})^{-1} U_\beta$，我们发现这与代码 `OrdinalScoreTest_fast` 中的计算完全吻合：
*   `U_G = crossprod(G, r)`  ($G^T r$)
*   `I_GG = G_t_W %*% G`  ($G^T W G$)
*   `I_GZ = G_t_W %*% Z`  ($G^T W Z$)
*   `I_ZZ_inv`  ($(Z^T W Z)^{-1}$)
*   `var_U_G = I_GG - I_GZ %*% I_ZZ_inv %*% t(I_GZ)`  ($(I_{\beta\beta} - I_{\beta\gamma}I_{\gamma\gamma}^{-1}I_{\gamma\beta})$)
*   `score_stat = U_G^2 / var_U_G`  ($U_\beta^T [Var(U_\beta)]^{-1} U_\beta$)

### 对于有序多分类模型的具体化

您代码中最复杂的部分，就是为有序多分类模型推导出具体的 $r_i$ 和 $W_i$ 的计算公式。这个过程需要用到对向量值函数求导的链式法则，因为 $\pi_i$ 是一个向量，而 $\eta_i$ 是一个标量。

*   **导数矩阵 D**: $D_i = \frac{\partial \pi_i}{\partial \eta_i}$，这是一个 $K \times 1$ 的向量（在您的代码中是 $D_{mat}$ 的第 $i$ 行）。
*   **协方差矩阵 V**: $V_i = Cov(y_i)$，这是一个 $K \times K$ 的多项分布协方差矩阵。

经过推导，可以得到：
*   **广义残差 `r`**: $r_i = (D_i)^T V_i^{-1} (y_i - \pi_i)$
*   **权重 `W`**: $W_i = (D_i)^T V_i^{-1} D_i$

您的代码 `NullModel_Score_Logit_Robust` 第5部分正是这些公式的**高效向量化实现**。

### 总结

1.  **得分检验理论**: 是一种基于零模型拟合结果，通过评估完整模型对数似然函数的梯度和曲率来构造的检验，避免了拟合复杂模型。
2.  **广义残差 `r` 和权重 `W`**: 是GLM框架下得分函数和信息矩阵的具体数学表达。它们将原始的残差 $(y-\mu)$ 和模型的结构（链接函数、方差函数）结合在一起。
3.  **您的代码**: 成功地为**有序多分类模型**推导并实现了 `r` 和 `W` 的解析计算公式，并将其应用于得分检验，这是一个非常专业和高效的实现。









您提出了一个非常深刻的问题，触及了将标准GLM理论推广到更复杂模型时的核心差异。

答案是：**有序多分类模型的广义残差和权重，在概念上与标准GLM框架完全一致，但在数学形式和计算实现上要复杂得多。**

它们的不一样主要源于**响应变量的结构差异**：

*   **标准GLM**（如二元Logistic回归、Poisson回归）：每个观测值 $i$ 对应一个**标量（scalar）**响应 $y_i$ 和一个**标量**均值 $\mu_i$。
*   **有序多分类模型**: 每个观测值 $i$ 虽然只有一个类别响应（如 '中'），但在建模时，我们处理的是一个**包含K个类别概率的向量** $\pi_i = (\pi_{i1}, \pi_{i2}, \dots, \pi_{iK})$。

这个从**标量到向量**的转变，导致了广义残差和权重计算上的根本不同。

---

### 标准GLM框架下的广义残差和权重

我们回顾一下标准GLM（以二元Logistic回归为例）的情况，以便对比：

*   **响应 $y_i$**: 0 或 1 (标量)
*   **均值 $\mu_i$**: $P(y_i=1) = \pi_i$ (标量)
*   **方差 $Var(y_i)$**: $\pi_i(1-\pi_i)$ (标量)
*   **线性预测器 $\eta_i$**: $Z_i^T\gamma$ (标量)
*   **导数 $\frac{d\mu_i}{d\eta_i}$**: $\pi_i(1-\pi_i)$ (标量)

在这种情况下：

1.  **权重 $W_i$ (标量)**:
    $$
    W_i = \left(\frac{d\mu_i}{d\eta_i}\right)^2 \frac{1}{Var(y_i)} = \frac{[\pi_i(1-\pi_i)]^2}{\pi_i(1-\pi_i)} = \pi_i(1-\pi_i)
    $$
    它是一个简单的标量值。`W` 矩阵就是一个对角线上元素为 $W_i$ 的对角矩阵。

2.  **广义残差 $r_i$ (标量)**:
    $$
    r_i = (y_i - \mu_i) \frac{d\eta_i}{d\mu_i} = \frac{y_i - \pi_i}{\pi_i(1-\pi_i)}
    $$
    它也是一个标量，通常被称为“工作残差 (working residual)”。

**计算非常直接，不涉及复杂的矩阵运算。**

---

### 有序多分类模型下的广义残差和权重

现在我们转向有序多分类模型，情况变得复杂起来：

*   **响应 $y_i$**: 虽然观测值是一个类别，但我们用一个 $K \times 1$ 的**指示向量**来表示它，例如 $(0, 1, 0, \dots, 0)^T$。
*   **均值 $\mu_i$**: 这是一个 $K \times 1$ 的**概率向量** $\pi_i = (\pi_{i1}, \dots, \pi_{iK})^T$。
*   **方差 $Var(y_i)$**: 这是一个 $K \times K$ 的**==协方差矩阵==** $V_i$，即多项分布的协方差矩阵。它不是对角阵。
*   **线性预测器 $\eta_i$**: $Z_i^T\gamma$ (仍然是标量)。
*   **导数 $\frac{\partial \mu_i}{\partial \eta_i}$**: 这是一个 $K \times 1$ 的**==导数向量==** $D_i$。它的每个元素是 $\frac{\partial \pi_{ij}}{\partial \eta_i}$。

由于这些组件都变成了向量或矩阵，广义残差和权重的定义也必须相应地变为矩阵形式：

1.  **权重 $W_i$ (现在是标量，但由矩阵运算得到)**:
    $$
    W_i = (D_i)^T V_i^{-1} D_i
    $$
    *   **$D_i$**: 一个 $K \times 1$ 的向量。
    *   **$V_i^{-1}$**: 一个 $K \times K$ 的**逆协方差矩阵**。
    *   **$D_i^T$**: 一个 $1 \times K$ 的行向量。

    最终的 $W_i$ 依然是一个**标量**。但它的计算过程涉及到了**对每个样本的协方差矩阵求逆**，这在计算上非常昂贵。

2.  **广义残差 $r_i$ (现在是标量，但由矩阵运算得到)**:
    $$
    r_i = (D_i)^T V_i^{-1} (y_i - \mu_i)
    $$
    *   **$(y_i - \mu_i)$**: 这是一个 $K \times 1$ 的残差向量。
    
    与权重类似，最终的 $r_i$ 也是一个**标量**，但其计算过程同样复杂。

### 您代码中的巧妙之处

直接为每个样本 `i` 构建并求逆一个 $K \times K$ 的矩阵 $V_i$ 是非常低效的。您的 `NullModel_Score_Logit_Robust` 代码之所以高效，正是因为它利用了 $V_i^{-1}$ 的一个特殊数学性质：

对于多项分布的协方差矩阵 $V_i = \text{diag}(\pi_i) - \pi_i \pi_i^T$，它的逆矩阵 $V_i^{-1}$ 有一个简洁的结构：
$$
V_i^{-1} = \text{diag}(1/\pi_{i1}, \dots, 1/\pi_{iK}) + \frac{1}{\pi_{iK}} \mathbf{1}\mathbf{1}^T
$$
其中 $\mathbf{1}$ 是一个全1的列向量。

利用这个公式，任何形如 $V_i^{-1} \mathbf{v}$ 的矩阵向量乘法都可以被转化为**元素对应乘法和求和**，从而避免了显式的矩阵求逆。这正是您代码中这两行的数学原理：

```R
Vinv_res <- res_pi_mat * pi_inv + (rowSums(res_pi_mat) * pi_inv[, K])
Vinv_D   <- D_mat * pi_inv    + (rowSums(D_mat) * pi_inv[, K])
```

### 总结对比

| 特性 | 标准 GLM (e.g., Logistic) | 有序多分类 GLM |
| :--- | :--- | :--- |
| **响应/均值结构** | **标量 (Scalar)** | **向量 (Vector)** |
| **方差结构** | **标量 (Scalar)**, $Var(y_i)$ | **矩阵 (Matrix)**, $Cov(y_i)$ |
| **导数结构** | **标量 (Scalar)**, $\frac{d\mu_i}{d\eta_i}$ | **向量 (Vector)**, $D_i = \frac{\partial \pi_i}{\partial \eta_i}$ |
| **权重 W 的计算** | 简单标量运算: $W_i = (\frac{d\mu_i}{d\eta_i})^2 / Var(y_i)$ | **复杂的矩阵二次型**: $W_i = D_i^T V_i^{-1} D_i$ |
| **残差 r 的计算** | 简单标量运算: $r_i = (y_i-\mu_i) / \frac{d\mu_i}{d\eta_i}$ | **复杂的矩阵运算**: $r_i = D_i^T V_i^{-1} (y_i - \mu_i)$ |
| **最终形式** | `r` 和 `W` 都是 $N \times 1$ 的向量 | `r` 和 `W` 最终也是 $N \times 1$ 的向量 |

**结论**:

有序多分类的广义残差和权重**在概念上**与标准GLM框架是**一致的**，它们都源于对对数似然函数进行泰勒展开或求导后得到的得分函数和信息矩阵。

然而，由于有序多分类模型的响应变量在数学上被处理为**向量**，其**具体计算公式**就从简单的标量运算升级为**复杂的矩阵运算**。标准GLM可以看作是K=2时的一个特例，其中向量和矩阵运算可以被大幅简化。您提供的代码通过利用多项分布协方差矩阵逆的特殊结构，非常高效地实现了这些复杂的矩阵运算，这是其高性能的关键所在。







好的，这是一个非常核心的GLM理论问题。这个性质是广义线性模型之所以“广义”且优雅的关键之一。我们来详细解释为什么 $\frac{\partial \ell_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}$。

这个结论来源于GLM所基于的**指数分布族（Exponential Dispersion Family）**的数学形式。

### 1. 指数分布族的标准形式

一个概率分布如果属于指数分布族，其概率密度函数（或概率质量函数）$f(y_i; \theta_i, \phi)$ 可以写成以下标准形式：

$$
f(y_i; \theta_i, \phi) = \exp\left[ \frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right]
$$

这里：
*   $y_i$: 观测值。
*   $\theta_i$: **自然参数 (natural parameter)**。这是模型的“核心”参数，它直接与线性预测器 $\eta_i$ 相关（通常 $\eta_i = \theta_i$）。
*   $\phi$: **离散参数 (dispersion parameter)**。它与方差有关，对于某些分布（如二项分布和泊松分布），它是一个固定的常数（通常 $a(\phi)=1$）。
*   $a(\cdot), b(\cdot), c(\cdot)$: 是已知的函数，它们定义了具体的分布是哪一个（如正态分布、泊松分布等）。

**对数似然函数**就是对上面这个式子取对数：
$$
\ell_i = \log f(y_i; \theta_i, \phi) = \frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi)
$$

### 2. 指数分布族的重要性质

指数分布族有两个非常优美的数学性质，它们是通过对 $b(\theta_i)$ 函数求导得到的：

*   **性质1：均值 (Expectation)**
    $$
    E(y_i) = \mu_i = b'(\theta_i) = \frac{d b(\theta_i)}{d \theta_i}
    $$
    也就是说，响应变量的期望值 $\mu_i$ 是函数 $b(\cdot)$ 对自然参数 $\theta_i$ 的一阶导数。

*   **性质2：方差 (Variance)**
    $$
    Var(y_i) = b''(\theta_i) a(\phi) = \frac{d^2 b(\theta_i)}{d \theta_i^2} a(\phi)
    $$
    响应变量的方差是 $b(\cdot)$ 的二阶导数乘以离散参数函数 $a(\phi)$。

### 3. 推导 $\frac{\partial \ell_i}{\partial \mu_i}$

现在，我们有了所有需要的工具。我们要计算的是对数似然 $\ell_i$ 对**均值 $\mu_i$** 的偏导数。这是一个链式法则问题：

$$
\frac{\partial \ell_i}{\partial \mu_i} = \frac{\partial \ell_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i}
$$

我们来分别计算这两部分：

#### **第一部分: $\frac{\partial \ell_i}{\partial \theta_i}$ (对自然参数求导)**

我们对对数似然函数 $\ell_i$ 直接关于 $\theta_i$ 求导：
$$
\frac{\partial \ell_i}{\partial \theta_i} = \frac{\partial}{\partial \theta_i} \left[ \frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right]
$$
因为 $y_i$ 和 $\phi$ 被视为常数，所以 $c(y_i, \phi)$ 的导数为0。我们得到：
$$
\frac{\partial \ell_i}{\partial \theta_i} = \frac{y_i - b'(\theta_i)}{a(\phi)}
$$
现在，利用**性质1** ($b'(\theta_i) = \mu_i$)，我们可以把上式替换为：
$$
\frac{\partial \ell_i}{\partial \theta_i} = \frac{y_i - \mu_i}{a(\phi)}
$$

#### **第二部分: $\frac{\partial \theta_i}{\partial \mu_i}$ (自然参数对均值求导)**

这部分是第一部分的逆运算。我们知道 $b'(\theta_i) = \mu_i$。根据反函数求导法则，我们先求 $\frac{\partial \mu_i}{\partial \theta_i}$：
$$
\frac{\partial \mu_i}{\partial \theta_i} = \frac{\partial}{\partial \theta_i} (b'(\theta_i)) = b''(\theta_i)
$$
所以，我们要求的导数就是它的倒数：
$$
\frac{\partial \theta_i}{\partial \mu_i} = \frac{1}{b''(\theta_i)}
$$

#### **组合两部分**

现在，我们将两部分乘起来：
$$
\frac{\partial \ell_i}{\partial \mu_i} = \frac{\partial \ell_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i} = \left( \frac{y_i - \mu_i}{a(\phi)} \right) \cdot \left( \frac{1}{b''(\theta_i)} \right) = \frac{y_i - \mu_i}{b''(\theta_i) a(\phi)}
$$
最后，我们观察分母 $b''(\theta_i) a(\phi)$。根据**性质2**，这正是 $Var(y_i)$！

所以，我们最终得到了这个优美的结论：
$$
\frac{\partial \ell_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}
$$

### 结论

这个性质是指数分布族与生俱来的内在数学结构所决定的。它之所以重要，是因为它揭示了一个深刻的联系：

**对数似然函数关于均值的梯度，正比于“残差”($y_i - \mu_i$)，并由方差进行了标准化。**

这使得GLM的得分函数和信息矩阵具有非常整洁和统一的形式，这也是为什么GLM的拟合算法（如IRLS）和得分检验理论能够适用于所有属于指数分布族的分布（正态、二项、泊松、伽马等）的原因。这个性质是连接不同分布的桥梁，是GLM理论的基石。






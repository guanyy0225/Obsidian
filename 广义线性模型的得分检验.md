
### 广义线性模型 (GLM) 的得分检验理论

得分检验（Score Test）是一种在**零假设 ($H_0$)** 成立的条件下构建的检验方法。它的核心思想是：**如果零假设是正确的，那么在零假设参数下的最大似然估计处，完整模型的对数似然函数关于“备择假设中新增参数”的梯度（斜率）应该接近于零。**

#### **数学设定**

假设我们的完整模型（Full Model）参数为 $\theta = (\beta, \gamma)$，其中 $\beta$ 是我们感兴趣的参数（例如，新基因型的效应），而 $\gamma$ 是零模型中已有的参数（例如，截距和协变量的效应）。

我们要检验的原假设是 $H_0: \beta = \beta_0$ （通常是 $\beta_0=0$）。

1.  **对数似然函数**: 记为 $\ell(\theta) = \ell(\beta, \gamma)$。
2.  **得分函数 (Score Function)**: 是对数似然函数关于所有参数的**一阶偏导数（梯度）**向量：
    $$
    U(\theta) = \frac{\partial \ell(\theta)}{\partial \theta} = \begin{pmatrix} U_\beta(\theta) \\ U_\gamma(\theta) \end{pmatrix}
    $$
3.  **信息矩阵 (Information Matrix)**: 是得分函数方差-协方差矩阵，等于负的对数似然函数**二阶偏导数（Hessian矩阵）**的期望：
    $$
    I(\theta) = -E\left[\frac{\partial^2 \ell(\theta)}{\partial \theta \partial \theta^T}\right] = \begin{pmatrix} I_{\beta\beta} & I_{\beta\gamma} \\ I_{\gamma\beta} & I_{\gamma\gamma} \end{pmatrix}
    $$

#### **得分检验的推导**

1.  **只拟合零模型**: 在 $H_0: \beta = 0$ 的条件下，我们最大化 $\ell(0, \gamma)$，得到 $\gamma$ 的估计值，记为 $\tilde{\gamma}$。
2.  **评估得分**: 我们在点 $\tilde{\theta} = (0, \tilde{\gamma})$ 处评估**完整模型**的得分函数 $U(\tilde{\theta})$。
    *   根据最大似然估计的性质，由于 $\tilde{\gamma}$ 是零模型的最优解，所以 $U_\gamma(0, \tilde{\gamma}) = 0$。
    *   因此，得分向量在 $\tilde{\theta}$ 处变为：
        $$
        U(\tilde{\theta}) = \begin{pmatrix} U_\beta(0, \tilde{\gamma}) \\ 0 \end{pmatrix}
        $$
        我们只需要关心 $U_\beta(0, \tilde{\gamma})$，这部分通常简记为 $U_\beta$。
3.  **得分统计量**: 在 $H_0$ 下，$U(\tilde{\theta})$ 近似服从均值为0，协方差矩阵为 $I(\tilde{\theta})$ 的正态分布。得分检验统计量 $S$ 是其二次型：
    $$
    S = U(\tilde{\theta})^T [I(\tilde{\theta})]^{-1} U(\tilde{\theta})
    $$
    利用分块矩阵求逆公式，可以证明这个统计量等价于：
    $$
    S = U_\beta^T (I_{\beta\beta} - I_{\beta\gamma}I_{\gamma\gamma}^{-1}I_{\gamma\beta})^{-1} U_\beta
    $$
    其中所有信息矩阵块都在 $\tilde{\theta}$ 处评估。在 $H_0$ 下，$S$ 近似服从自由度为 $\text{dim}(\beta)$ 的卡方分布。

### 广义残差 (r) 和权重矩阵 (W) 的数学原理

得分函数是 **对数似然函数 (log-likelihood) 对我们感兴趣的参数 $\beta$ 的一阶偏导数**。我们用 $l$ 表示总的对数似然函数，它等于每个观测值的对数似然 $l_i$ 之和：
$$
l = \sum_{i=1}^n l_i
$$
因此，得分函数为：
$$
U_\beta = \frac{\partial l}{\partial \beta} = \sum_{i=1}^n \frac{\partial l_i}{\partial \beta}
$$
现在，我们的任务就是计算出 $\frac{\partial l_i}{\partial \beta}$。

### 第一步：运用链式法则

参数 $\beta$ 并非直接出现在 $l_i$ 中。它们之间的关系是这样的：
1.  对数似然 $l_i$ 是关于 $y_i$ 的均值 $\mu_i$ 的函数。
2.  均值 $\mu_i$ 是通过**反向链接函数**与线性预测子 $\eta_i$ 关联的，即 $\mu_i = g^{-1}(\eta_i)$。
3.  线性预测子 $\eta_i$ 是我们参数 $\beta$ 和协变量 $G_i$ 的线性组合，即 $\eta_i = Z_i^T \gamma + G_i^T \beta$。

因此，为了求 $\frac{\partial l_i}{\partial \beta}$，我们必须使用链式法则：

$$
\frac{\partial l_i}{\partial \beta} = \frac{\partial l_i}{\partial \mu_i} \cdot \frac{\partial \mu_i}{\partial \eta_i} \cdot \frac{\partial \eta_i}{\partial \beta}
$$

我们来逐一分解这三个部分。

### 第二步：分解链式法则的三个部分

#### 1. 计算 $\frac{\partial \eta_i}{\partial \beta}$

这是最简单的一部分。根据线性预测子的定义：
$$
\eta_i = Z_i^T \gamma + G_i^T \beta
$$
在零模型 ($H_0$) 下，我们虽然假设 $\beta=0$，但在求导时，我们仍然要对它求偏导。很明显：
$$
\frac{\partial \eta_i}{\partial \beta} = G_i
$$

#### 2. 计算 $\frac{\partial l_i}{\partial \mu_i}$

这一部分是GLM理论中的一个经典结果。对于指数族分布，可以证明一个非常优美的关系：
$$
\frac{\partial l_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}
$$
这个结果来自于对数似然函数对规范参数 $\theta$ 求导，然后再转换到对均值 $\mu$ 求导。它把复杂的求导变成了一个非常直观的形式：**（观测值 - 期望值）/ 方差**。这本质上是一个标准化的残差。

#### 3. $\frac{\partial \mu_i}{\partial \eta_i}$ 这一项

这一项就是反向链接函数 $g^{-1}$ 对线性预测子 $\eta_i$ 的导数。我们暂时就保留这个形式。

### 第三步：组合所有部分

现在，我们将上面得到的三个部分代回到链式法则的公式中：

$$
\frac{\partial l_i}{\partial \beta} = \underbrace{\left( \frac{y_i - \mu_i}{Var(y_i)} \right)}_{\frac{\partial l_i}{\partial \mu_i}} \cdot \underbrace{\left( \frac{\partial \mu_i}{\partial \eta_i} \right)}_{\frac{\partial \mu_i}{\partial \eta_i}} \cdot \underbrace{\left( G_i \right)}_{\frac{\partial \eta_i}{\partial \beta}}
$$

重新整理一下顺序，把 $G_i$ 放在前面：

$$
\frac{\partial l_i}{\partial \beta} = G_i \cdot \left( \frac{y_i - \mu_i}{Var(y_i)} \right) \cdot \left( \frac{\partial \mu_i}{\partial \eta_i} \right)
$$

总的得分函数 $U_\beta$ 是所有观测值的加和：

$$
U_\beta = \sum_{i=1}^n \frac{\partial l_i}{\partial \beta} = \sum_{i=1}^n G_i \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right)
$$

这正是您在问题中给出的第一个公式！到这里，我们已经完成了得分函数的基本推导。

### 第四步：引入广义残差 $r$ 和权重 $W$

现在，我们看看为什么引入 $r_i$ 和 $W_i$ 能让公式更简洁。回顾它们的定义（并且我们所有计算都是在零模型 $\tilde{\mu}_i$ 下进行）：

1.  **权重 $W_i$**: $W_i = \left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2 \frac{1}{Var(y_i)}$
2.  **广义残差 $r_i$**: $r_i = (y_i - \mu_i) \frac{\partial \eta_i}{\partial \mu_i}$

注意，$\frac{\partial \eta_i}{\partial \mu_i} = 1 / \left(\frac{\partial \mu_i}{\partial \eta_i}\right)$。所以 $r_i$ 也可以写成：
$$
r_i = \frac{y_i - \mu_i}{\partial \mu_i / \partial \eta_i}
$$

让我们看看如何从 $U_\beta$ 的表达式中凑出这两个量。

我们从上一步得到的 $U_\beta$ 表达式开始：
$$
U_\beta = \sum_{i=1}^n G_i \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right)
$$

现在，我们对括号里的部分做一个巧妙的代数变换：**分子分母同时乘以 $\frac{\partial \mu_i}{\partial \eta_i}$**。

$$
U_\beta = \sum_{i=1}^n G_i \left( \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)} \cdot \frac{y_i - \mu_i}{\frac{\partial \mu_i}{\partial \eta_i}} \right)
$$

仔细观察这个新形式，我们发现：
*   第一部分 $\frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}$ 正是**权重 $W_i$** 的定义！
*   第二部分 $\frac{y_i - \mu_i}{\frac{\partial \mu_i}{\partial \eta_i}}$ 正是**广义残差 $r_i$** 的定义！

所以，我们可以把 $U_\beta$ 写成：
$$
U_\beta = \sum_{i=1}^n G_i \cdot W_i \cdot r_i
$$

如果我们将 $G$ 看作是 $n \times q$ 的设计矩阵（$q$ 是 $\beta$ 的维度），$W$ 是一个对角线上为 $W_i$ 的 $n \times n$ 对角矩阵，$r$ 是一个包含 $r_i$ 的 $n \times 1$ 的向量，那么上面的求和可以写成矩阵形式：

$$
U_\beta = G^T W r
$$

### 总结与解释

1.  **得分函数 $U_\beta$ 的本质**：它衡量了在零模型（即不包含 $G$ 的模型）下，**新协变量 $G$ 与模型残差之间的“相关性”**。如果 $G$ 与残差高度相关，说明 $G$ 可以解释当前模型无法解释的变异，那么 $G$ 应该被加入模型，$\beta$ 不为零。

2.  **为什么是“广义”残差 $r$**：它不是简单的 $y_i - \mu_i$。它还除以了 $\frac{\partial \mu_i}{\partial \eta_i}$。这一步的作用是 **将残差从原始响应变量 $y$ 的尺度，转换到线性预测子 $\eta$ 的尺度**。这使得不同链接函数下的残差具有可比性，所以它被称为“工作因变量”或“广义残差”。

3.  **为什么需要权重 $W$**：权重 $W_i$ 同时考虑了两个因素：
    *   **方差 $Var(y_i)$**：方差大的观测值（信息量少，不稳定）应该被赋予较小的权重。
    *   **链接函数的影响 $(\frac{\partial \mu_i}{\partial \eta_i})^2$**：这个导数衡量了线性预测子 $\eta_i$ 的微小变化能引起均值 $\mu_i$ 多大的变化。在某些区域（例如 logistic 回归中概率接近 0 或 1 的地方），这个导数很小，说明模型在该处非常“不敏感”，因此这些观测值也应被赋予较小的权重。

最终得到的 $U_\beta = G^T W r$ 是一个非常优雅和强大的形式。它告诉我们，**得分统计量本质上是新协变量 $G$ 与经过方差和链接函数校正后的广义残差 $r$ 之间的加权和（或加权相关性）**。

您代码中计算的 `residuals`，如果最终用于构建得分检验，那么它很可能就是这个 $r_i$ (广义残差) 或者 $W_i r_i$ (加权广义残差) 的具体实现。




## 信息矩阵

我们的目标是计算这个 Hessian 矩阵，然后取其负期望，最后将结果表示成 $X^T W X$ 的形式。

### 第一步：回顾一阶导数（得分函数）

在之前的推导中，我们得到了单个观测值 $l_i$ 对参数 $\theta$ 的一阶导数：
$$
\frac{\partial l_i}{\partial \theta} = \frac{\partial \eta_i}{\partial \theta} \cdot \frac{\partial \mu_i}{\partial \eta_i} \cdot \frac{\partial l_i}{\partial \mu_i}
$$
我们知道 $\frac{\partial \eta_i}{\partial \theta}$ 是设计矩阵的第 $i$ 行 $X_i^T$，并且 $\frac{\partial l_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}$。所以：
$$
\frac{\partial l_i}{\partial \theta} = X_i^T \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right)
$$
总的得分函数是 $U = \sum_{i=1}^n \frac{\partial l_i}{\partial \theta}$。

### 第二步：计算二阶导数（Hessian 矩阵）

现在，我们要对上面的得分函数再求一次导数，得到 Hessian 矩阵 $H = \frac{\partial^2 l}{\partial \theta \partial \theta^T}$。
$$
H = \frac{\partial U}{\partial \theta^T} = \sum_{i=1}^n \frac{\partial}{\partial \theta^T} \left[ X_i^T \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right) \right]
$$
这是一个 $p \times p$ 的矩阵（其中 $p$ 是参数总数）。我们可以把它写成 $\sum X_i^T (\dots) X_i$ 的形式。我们用链式法则来处理括号里的部分，注意这里是对行向量 $\theta^T$ 求导，实际上等价于对 $\eta_i$ 求导再乘以 $\frac{\partial \eta_i}{\partial \theta^T} = X_i$：
$$
H = \sum_{i=1}^n X_i^T \left[ \frac{\partial}{\partial \eta_i} \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right) \right] X_i
$$
现在我们来处理中间的导数项，这需要用到乘法法则：
$$
\frac{\partial}{\partial \eta_i} \left( \dots \right) = \underbrace{\frac{\partial}{\partial \eta_i} \left(\frac{\partial \mu_i}{\partial \eta_i} \frac{1}{Var(y_i)}\right) \cdot (y_i - \mu_i)}_{\text{Term A}} + \underbrace{\left(\frac{\partial \mu_i}{\partial \eta_i} \frac{1}{Var(y_i)}\right) \cdot \frac{\partial (y_i - \mu_i)}{\partial \eta_i}}_{\text{Term B}}
$$
-   **Term A**: 这是一个很复杂的项，它依赖于链接函数和方差函数对 $\eta_i$ 的二阶导数。
-   **Term B**: 这一项比较简单。因为 $\frac{\partial y_i}{\partial \eta_i} = 0$（$y_i$ 是常数），所以 $\frac{\partial (y_i - \mu_i)}{\partial \eta_i} = - \frac{\partial \mu_i}{\partial \eta_i}$。
    因此，Term B = $-\left(\frac{\partial \mu_i}{\partial \eta_i} \frac{1}{Var(y_i)}\right) \cdot \frac{\partial \mu_i}{\partial \eta_i} = - \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}$。

把它们代回去，Hessian 矩阵就是：
$$
H = \sum_{i=1}^n X_i^T \left[ \text{Term A} \cdot (y_i - \mu_i) - \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)} \right] X_i
$$

### 第三步：取负期望值（关键的简化步骤）

现在我们要计算信息矩阵 $I = -E[H]$。期望是作用在随机变量 $y_i$ 上的。
$$
I = -E \left[ \sum_{i=1}^n X_i^T \left[ \text{Term A} \cdot (y_i - \mu_i) - \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)} \right] X_i \right]
$$
根据期望的线性性质，我们可以把求和与期望交换顺序：
$$
I = \sum_{i=1}^n X_i^T \left[ -E[\text{Term A} \cdot (y_i - \mu_i)] + E\left[\frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}\right] \right] X_i
$$
这里就是奇迹发生的地方：
1.  **对于第一部分**：$E[y_i - \mu_i] = E[y_i] - \mu_i = \mu_i - \mu_i = 0$。因为 Term A 部分不依赖于随机变量 $y_i$，所以 $E[\text{Term A} \cdot (y_i - \mu_i)] = \text{Term A} \cdot E[y_i - \mu_i] = 0$。
    **这个复杂的 Term A 在取期望后就消失了！** 这就是为什么我们使用期望信息矩阵（Fisher Information）而不是观测信息矩阵（Observed Information，即Hessian本身）的原因之一，它极大地简化了计算。

2.  **对于第二部分**：$\frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}$ 这个表达式中所有项都只依赖于均值 $\mu_i$（方差通常是均值的函数），而不依赖于随机的 $y_i$。因此，它对于期望算子来说是常数。
    $E\left[\frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}\right] = \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}$。

这正是我们之前定义的**权重 $W_i$**！
$$
W_i = \left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2 \frac{1}{Var(y_i)}
$$

所以，整个表达式简化为：
$$
I = \sum_{i=1}^n X_i^T W_i X_i
$$

### 第四步：写成矩阵形式并分块

这个求和 $\sum_{i=1}^n X_i^T W_i X_i$ 正是矩阵乘法 **$X^T W X$** 的定义，其中 $X$ 是完整的 $n \times p$ 设计矩阵，$W$ 是一个 $n \times n$ 的对角矩阵，对角线上的元素是 $W_1, W_2, \dots, W_n$。

所以我们得到了核心结果：
$$
I \approx X^T W X
$$
（这里的“约等于”表示我们用期望信息矩阵近似了观测信息矩阵）

现在，我们将完整的设计矩阵 $X$ 和参数 $\theta$ 分块：
$$
X = [Z, G] \quad \text{和} \quad \theta = \begin{pmatrix} \gamma \\ \beta \end{pmatrix}
$$
信息矩阵 $I$ 也相应地分块：
$$
I = \begin{pmatrix} I_{\gamma\gamma} & I_{\gamma\beta} \\ I_{\beta\gamma} & I_{\beta\beta} \end{pmatrix}
$$
我们将分块的 $X$ 代入 $I = X^T W X$：
$$
I = \begin{pmatrix} Z^T \\ G^T \end{pmatrix} W \begin{pmatrix} Z & G \end{pmatrix}
$$
执行这个分块矩阵乘法：
$$
I = \begin{pmatrix}
Z^T W Z & Z^T W G \\
G^T W Z & G^T W G
\end{pmatrix}
$$
通过直接比较两个分块矩阵，我们就得到了您问题中的结论：
$$
I_{\gamma\gamma} \approx Z^T W Z
$$
$$
I_{\beta\beta} \approx G^T W G
$$
$$
I_{\beta\gamma} \approx G^T W Z
$$

### 结论与直觉

这个结果非常漂亮，它告诉我们，在GLM框架下，信息矩阵（它决定了参数估计的方差）可以被看作是**设计矩阵的加权交叉积**，形式上与普通最小二乘法中的 $(X^T X)$ 非常相似。

-   在OLS中，信息矩阵（的某个缩放）是 $X^T X$。
-   在GLM中，它是 $X^T W X$。

**权重矩阵 $W$** 起到了关键的调节作用，它为每个观测值赋予了不同的“信息量”。一个观测值的权重 $W_i$ 大，意味着：
1.  它的**方差 $Var(y_i)$ 比较小**（数据点更稳定，信息量更大）。
2.  它所处的位置让**均值 $\mu_i$ 对线性预测子 $\eta_i$ 的变化更敏感**（即 $\frac{\partial \mu_i}{\partial \eta_i}$ 比较大）。

这个 $X^T W X$ 矩阵不仅是理论上的信息矩阵，它也是**拟合GLM的“迭代重加权最小二乘法”（IRLS）算法**中每一步迭代的核心。算法在每一步都会根据当前的参数估计更新权重 $W$，然后求解一个加权最小二乘问题来更新参数。









非常棒！您已经把广义线性模型（GLM）得分检验的理论基础、推导过程以及核心组件（广义残差 `r` 和权重 `W`）总结得非常清晰和准确了。这为我们对比有序多分类模型的得分检验打下了完美的基础。

现在，我们来回答您的核心问题：**广义线性模型的得分检验与有序多分类的得分检验有什么不同？**

**核心答案：** 理论框架完全相同，但**具体实现**（即似然函数、得分函数、信息矩阵的具体形式）有本质区别。这种区别源于两种模型对“响应变量”和“模型结构”的根本性不同定义。

GLM处理的是单个响应变量（$y_i$），而有序多分类模型处理的是一个具有$K$个类别的响应变量，其核心是**对多个累积概率建模**。这导致了得分和信息矩阵的结构变得更加复杂。

下面我们来逐一对比，看看有序多分类模型是如何改变这些组件的。

---

### 对比分析：GLM vs. 有序多分类模型

| 特征 | 标准广义线性模型 (GLM) | 有序多分类模型 (Ordinal Model) | 关键区别 |
| :--- | :--- | :--- | :--- |
| **1. 似然函数** | 基于单个响应 $y_i$ 的概率（或密度）函数，如二项分布、泊松分布等。$l_i = \log P(Y_i=y_i)$. | 基于**多项分布**。对于观测 $i$ 属于类别 $k$ ($y_i=k$)，其似然是 $P(y_i=k) = P(Y_i \le k) - P(Y_i \le k-1)$。 | GLM对单个均值 $\mu_i$ 建模；有序模型对 **$K-1$ 个累积概率**建模。 |
| **2. 模型结构** | 单个线性预测子：<br>$\eta_i = Z_i^T\gamma + G_i^T\beta$ <br> $\mu_i = g^{-1}(\eta_i)$ | **多个**线性预测子，但结构特殊：<br> $\text{logit}(P(Y_i \le j)) = \alpha_j - (Z_i^T\gamma + G_i^T\beta)$ <br> (for $j=1, \dots, K-1$) | GLM中每个观测 $i$ 只有一个 $\eta_i$。有序模型中每个观测 $i$ 对应 **$K-1$ 个**与类别相关的线性预测子。注意：**斜率 $\gamma, \beta$ 是跨类别共享的**（比例优势假设），但截距 $\alpha_j$ 是类别特有的。 |
| **3. 残差的概念** | **单个广义残差 $r_i$**：<br>$r_i = \frac{y_i - \mu_i}{\partial\mu_i/\partial\eta_i}$ <br> 这是一个标量。 | **残差向量**：对每个观测 $i$（假设其真实类别为 $k$），我们可以定义一个“残差向量”。例如，一个长度为 $K-1$ 的向量，其第 $j$ 个元素为：$e_{ij} = I(y_i \le j) - P(Y_i \le j)$，其中 $I(\cdot)$ 是指示函数。 | 从**标量残差**变成了**向量残差**。一个观测值的“误差”体现在它与所有 $K-1$ 个累积概率的偏差上，而不仅仅是一个均值。 |
| **4. 权重的概念** | **单个权重 $W_i$**：<br>$W_i = (\frac{\partial\mu_i}{\partial\eta_i})^2 \frac{1}{\text{Var}(y_i)}$ <br> 是一个标量。$W$ 是一个对角矩阵。 | **权重矩阵**：对于每个观测 $i$，其权重不再是标量，而是一个 **$(K-1) \times (K-1)$ 的协方差矩阵**。这个矩阵描述了该观测的 $K-1$ 个“残差”元素之间的方差和协方差。整个模型的权重矩阵 $W$ 是一个**块对角矩阵**，每个对角块是单个观测的权重矩阵。 | 从**对角权重矩阵**（对角线元素是标量 $W_i$）变成了**块对角权重矩阵**（对角线元素是矩阵）。 |
| **5. 得分函数 $U_\beta$** | $U_\beta = \sum_i G_i W_i r_i = G^T W r$ | $U_\beta = \sum_i G_i^T \cdot (\text{一些复杂的函数})$ <br> 这个函数涉及到对所有 $K-1$ 个累积概率求导，并根据观测的真实类别 $k$ 进行加权。其本质上仍然是**新协变量 $G_i$ 与残差向量的加权组合**，但形式复杂得多。 | 结构上仍然是 $\sum G \times \text{残差}$，但这里的残差和权重是向量和矩阵形式，计算更复杂。 |
| **6. 信息矩阵** | $I_{\beta\beta} \approx G^T W G$ <br> (W是对角矩阵) | $I_{\beta\beta} \approx \sum_i G_i^T W_i G_i$ <br> (这里的 $W_i$ 是观测 $i$ 的 $(K-1) \times (K-1)$ 权重矩阵，所以这是一个矩阵三明治乘积的和) | GLM的信息矩阵计算是简单的矩阵乘法。有序模型的信息矩阵需要对每个观测值的**矩阵三明治乘积进行求和**，计算量更大。 |

---

### 深入理解差异

让我们用一个更直观的方式来理解这个转变：

1.  **从一维到多维**：
    *   GLM的得分检验可以看作是在一维空间中进行的。每个观测 $i$ 贡献一个残差 $r_i$ 和一个权重 $W_i$。我们检验的是协变量 $G$ 是否与这个一维的残差序列相关。
    *   有序多分类模型的得分检验是在一个高维空间中进行的。每个观测 $i$ 贡献一个**残差向量**和一个**权重矩阵**。我们检验的是协变量 $G$ 是否与这个多维的残差序列相关联。

2.  **“残差”的含义变了**：
    *   在GLM中，$y_i - \mu_i$ 直观地表示了“观测值偏离期望值多少”。
    *   在有序模型中，残差不再这么简单。假设一个5分类问题，观测 $i$ 的真实类别是3。模型的预测是一组概率 $(p_1, p_2, p_3, p_4, p_5)$。这个观测的“误差”体现在：
        *   $P(Y \le 1)$ 的估计值与真实值（0）的差距。
        *   $P(Y \le 2)$ 的估计值与真实值（0）的差距。
        *   $P(Y \le 3)$ 的估计值与真实值（1）的差距。
        *   $P(Y \le 4)$ 的估计值与真实值（1）的差距。
    这些偏差共同构成了该观测的残差信息。

3.  **“权重”的含义也变了**：
    *   在GLM中，$W_i$ 衡量了观测 $i$ 的信息量。
    *   在有序模型中，权重矩阵 $W_i$ 不仅衡量了每个累积概率估计的方差（对角线元素），还衡量了**不同累积概率估计之间的协方差**（非对角线元素）。这是因为 $P(Y \le j)$ 和 $P(Y \le j+1)$ 必然是相关的，这种相关性必须在信息矩阵中被考虑。

### 总结

尽管数学形式变得复杂，但得分检验的**哲学思想**保持不变：

-   **GLM得分检验问**：“在我只用协变量 $Z$ 拟合的模型产生的残差中，是否还留有与新协变量 $G$ 相关的、未被解释的信息？”
-   **有序模型得分检验问**：“在我只用协变量 $Z$ 拟合的、描述类别累积概率的模型产生的（向量）残差中，是否还留有与新协变量 $G$ 相关的、未被解释的信息？”

因此，您的代码在实现有序多分类模型的得分检验时，虽然也需要计算“残差”和“权重”，但这些步骤会比标准的GLM复杂得多，需要处理向量和矩阵运算，而不是简单的标量运算。




您提出了一个非常深刻的问题，触及了将标准GLM理论推广到更复杂模型时的核心差异。

答案是：**有序多分类模型的广义残差和权重，在概念上与标准GLM框架完全一致，但在数学形式和计算实现上要复杂得多。**

它们的不一样主要源于**响应变量的结构差异**：

*   **标准GLM**（如二元Logistic回归、Poisson回归）：每个观测值 $i$ 对应一个**标量（scalar）**响应 $y_i$ 和一个**标量**均值 $\mu_i$。
*   **有序多分类模型**: 每个观测值 $i$ 虽然只有一个类别响应（如 '中'），但在建模时，我们处理的是一个**包含K个类别概率的向量** $\pi_i = (\pi_{i1}, \pi_{i2}, \dots, \pi_{iK})$。

这个从**标量到向量**的转变，导致了广义残差和权重计算上的根本不同。

---

### 标准GLM框架下的广义残差和权重

我们回顾一下标准GLM（以二元Logistic回归为例）的情况，以便对比：

*   **响应 $y_i$**: 0 或 1 (标量)
*   **均值 $\mu_i$**: $P(y_i=1) = \pi_i$ (标量)
*   **方差 $Var(y_i)$**: $\pi_i(1-\pi_i)$ (标量)
*   **线性预测器 $\eta_i$**: $Z_i^T\gamma$ (标量)
*   **导数 $\frac{d\mu_i}{d\eta_i}$**: $\pi_i(1-\pi_i)$ (标量)

在这种情况下：

1.  **权重 $W_i$ (标量)**:
    $$
    W_i = \left(\frac{d\mu_i}{d\eta_i}\right)^2 \frac{1}{Var(y_i)} = \frac{[\pi_i(1-\pi_i)]^2}{\pi_i(1-\pi_i)} = \pi_i(1-\pi_i)
    $$
    它是一个简单的标量值。`W` 矩阵就是一个对角线上元素为 $W_i$ 的对角矩阵。

2.  **广义残差 $r_i$ (标量)**:
    $$
    r_i = (y_i - \mu_i) \frac{d\eta_i}{d\mu_i} = \frac{y_i - \pi_i}{\pi_i(1-\pi_i)}
    $$
    它也是一个标量，通常被称为“工作残差 (working residual)”。

**计算非常直接，不涉及复杂的矩阵运算。**

---

### 有序多分类模型下的广义残差和权重

现在我们转向有序多分类模型，情况变得复杂起来：

*   **响应 $y_i$**: 虽然观测值是一个类别，但我们用一个 $K \times 1$ 的**指示向量**来表示它，例如 $(0, 1, 0, \dots, 0)^T$。
*   **均值 $\mu_i$**: 这是一个 $K \times 1$ 的**概率向量** $\pi_i = (\pi_{i1}, \dots, \pi_{iK})^T$。
*   **方差 $Var(y_i)$**: 这是一个 $K \times K$ 的**==协方差矩阵==** $V_i$，即多项分布的协方差矩阵。它不是对角阵。
*   **线性预测器 $\eta_i$**: $Z_i^T\gamma$ (仍然是标量)。
*   **导数 $\frac{\partial \mu_i}{\partial \eta_i}$**: 这是一个 $K \times 1$ 的**==导数向量==** $D_i$。它的每个元素是 $\frac{\partial \pi_{ij}}{\partial \eta_i}$。

由于这些组件都变成了向量或矩阵，广义残差和权重的定义也必须相应地变为矩阵形式：

1.  **权重 $W_i$ (现在是标量，但由矩阵运算得到)**:
    $$
    W_i = (D_i)^T V_i^{-1} D_i
    $$
    *   **$D_i$**: 一个 $K \times 1$ 的向量。
    *   **$V_i^{-1}$**: 一个 $K \times K$ 的**逆协方差矩阵**。
    *   **$D_i^T$**: 一个 $1 \times K$ 的行向量。

    最终的 $W_i$ 依然是一个**标量**。但它的计算过程涉及到了**对每个样本的协方差矩阵求逆**，这在计算上非常昂贵。

2.  **广义残差 $r_i$ (现在是标量，但由矩阵运算得到)**:
    $$
    r_i = (D_i)^T V_i^{-1} (y_i - \mu_i)
    $$
    *   **$(y_i - \mu_i)$**: 这是一个 $K \times 1$ 的残差向量。
    
    与权重类似，最终的 $r_i$ 也是一个**标量**，但其计算过程同样复杂。

### 您代码中的巧妙之处

直接为每个样本 `i` 构建并求逆一个 $K \times K$ 的矩阵 $V_i$ 是非常低效的。您的 `NullModel_Score_Logit_Robust` 代码之所以高效，正是因为它利用了 $V_i^{-1}$ 的一个特殊数学性质：

对于多项分布的协方差矩阵 $V_i = \text{diag}(\pi_i) - \pi_i \pi_i^T$，它的逆矩阵 $V_i^{-1}$ 有一个简洁的结构：
$$
V_i^{-1} = \text{diag}(1/\pi_{i1}, \dots, 1/\pi_{iK}) + \frac{1}{\pi_{iK}} \mathbf{1}\mathbf{1}^T
$$
其中 $\mathbf{1}$ 是一个全1的列向量。

利用这个公式，任何形如 $V_i^{-1} \mathbf{v}$ 的矩阵向量乘法都可以被转化为**元素对应乘法和求和**，从而避免了显式的矩阵求逆。这正是您代码中这两行的数学原理：

```R
Vinv_res <- res_pi_mat * pi_inv + (rowSums(res_pi_mat) * pi_inv[, K])
Vinv_D   <- D_mat * pi_inv    + (rowSums(D_mat) * pi_inv[, K])
```

### 总结对比

| 特性 | 标准 GLM (e.g., Logistic) | 有序多分类 GLM |
| :--- | :--- | :--- |
| **响应/均值结构** | **标量 (Scalar)** | **向量 (Vector)** |
| **方差结构** | **标量 (Scalar)**, $Var(y_i)$ | **矩阵 (Matrix)**, $Cov(y_i)$ |
| **导数结构** | **标量 (Scalar)**, $\frac{d\mu_i}{d\eta_i}$ | **向量 (Vector)**, $D_i = \frac{\partial \pi_i}{\partial \eta_i}$ |
| **权重 W 的计算** | 简单标量运算: $W_i = (\frac{d\mu_i}{d\eta_i})^2 / Var(y_i)$ | **复杂的矩阵二次型**: $W_i = D_i^T V_i^{-1} D_i$ |
| **残差 r 的计算** | 简单标量运算: $r_i = (y_i-\mu_i) / \frac{d\mu_i}{d\eta_i}$ | **复杂的矩阵运算**: $r_i = D_i^T V_i^{-1} (y_i - \mu_i)$ |
| **最终形式** | `r` 和 `W` 都是 $N \times 1$ 的向量 | `r` 和 `W` 最终也是 $N \times 1$ 的向量 |

**结论**:

有序多分类的广义残差和权重**在概念上**与标准GLM框架是**一致的**，它们都源于对对数似然函数进行泰勒展开或求导后得到的得分函数和信息矩阵。

然而，由于有序多分类模型的响应变量在数学上被处理为**向量**，其**具体计算公式**就从简单的标量运算升级为**复杂的矩阵运算**。标准GLM可以看作是K=2时的一个特例，其中向量和矩阵运算可以被大幅简化。您提供的代码通过利用多项分布协方差矩阵逆的特殊结构，非常高效地实现了这些复杂的矩阵运算，这是其高性能的关键所在。






### $\frac{\partial \ell_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}$的推导

这个结论来源于GLM所基于的**指数分布族（Exponential Dispersion Family）**的数学形式。

### 1. 指数分布族的标准形式

一个概率分布如果属于指数分布族，其概率密度函数（或概率质量函数）$f(y_i; \theta_i, \phi)$ 可以写成以下标准形式：

$$
f(y_i; \theta_i, \phi) = \exp\left[ \frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right]
$$

这里：
*   $y_i$: 观测值。
*   $\theta_i$: **自然参数 (natural parameter)**。这是模型的“核心”参数，它直接与线性预测器 $\eta_i$ 相关（通常 $\eta_i = \theta_i$）。
*   $\phi$: **离散参数 (dispersion parameter)**。它与方差有关，对于某些分布（如二项分布和泊松分布），它是一个固定的常数（通常 $a(\phi)=1$）。
*   $a(\cdot), b(\cdot), c(\cdot)$: 是已知的函数，它们定义了具体的分布是哪一个（如正态分布、泊松分布等）。

**对数似然函数**就是对上面这个式子取对数：
$$
\ell_i = \log f(y_i; \theta_i, \phi) = \frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi)
$$

### 2. 指数分布族的重要性质

指数分布族有两个非常优美的数学性质，它们是通过对 $b(\theta_i)$ 函数求导得到的：

*   **性质1：均值 (Expectation)**
    $$
    E(y_i) = \mu_i = b'(\theta_i) = \frac{d b(\theta_i)}{d \theta_i}
    $$
    也就是说，响应变量的期望值 $\mu_i$ 是函数 $b(\cdot)$ 对自然参数 $\theta_i$ 的一阶导数。

*   **性质2：方差 (Variance)**
    $$
    Var(y_i) = b''(\theta_i) a(\phi) = \frac{d^2 b(\theta_i)}{d \theta_i^2} a(\phi)
    $$
    响应变量的方差是 $b(\cdot)$ 的二阶导数乘以离散参数函数 $a(\phi)$。

### 3. 推导 $\frac{\partial \ell_i}{\partial \mu_i}$

现在，我们有了所有需要的工具。我们要计算的是对数似然 $\ell_i$ 对**均值 $\mu_i$** 的偏导数。这是一个链式法则问题：

$$
\frac{\partial \ell_i}{\partial \mu_i} = \frac{\partial \ell_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i}
$$

我们来分别计算这两部分：

#### **第一部分: $\frac{\partial \ell_i}{\partial \theta_i}$ (对自然参数求导)**

我们对对数似然函数 $\ell_i$ 直接关于 $\theta_i$ 求导：
$$
\frac{\partial \ell_i}{\partial \theta_i} = \frac{\partial}{\partial \theta_i} \left[ \frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right]
$$
因为 $y_i$ 和 $\phi$ 被视为常数，所以 $c(y_i, \phi)$ 的导数为0。我们得到：
$$
\frac{\partial \ell_i}{\partial \theta_i} = \frac{y_i - b'(\theta_i)}{a(\phi)}
$$
现在，利用**性质1** ($b'(\theta_i) = \mu_i$)，我们可以把上式替换为：
$$
\frac{\partial \ell_i}{\partial \theta_i} = \frac{y_i - \mu_i}{a(\phi)}
$$

#### **第二部分: $\frac{\partial \theta_i}{\partial \mu_i}$ (自然参数对均值求导)**

这部分是第一部分的逆运算。我们知道 $b'(\theta_i) = \mu_i$。根据反函数求导法则，我们先求 $\frac{\partial \mu_i}{\partial \theta_i}$：
$$
\frac{\partial \mu_i}{\partial \theta_i} = \frac{\partial}{\partial \theta_i} (b'(\theta_i)) = b''(\theta_i)
$$
所以，我们要求的导数就是它的倒数：
$$
\frac{\partial \theta_i}{\partial \mu_i} = \frac{1}{b''(\theta_i)}
$$

#### **组合两部分**

现在，我们将两部分乘起来：
$$
\frac{\partial \ell_i}{\partial \mu_i} = \frac{\partial \ell_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i} = \left( \frac{y_i - \mu_i}{a(\phi)} \right) \cdot \left( \frac{1}{b''(\theta_i)} \right) = \frac{y_i - \mu_i}{b''(\theta_i) a(\phi)}
$$
最后，我们观察分母 $b''(\theta_i) a(\phi)$。根据**性质2**，这正是 $Var(y_i)$！

所以，我们最终得到了这个优美的结论：
$$
\frac{\partial \ell_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}
$$

### 结论

这个性质是指数分布族与生俱来的内在数学结构所决定的。它之所以重要，是因为它揭示了一个深刻的联系：

**对数似然函数关于均值的梯度，正比于“残差”($y_i - \mu_i$)，并由方差进行了标准化。**

这使得GLM的得分函数和信息矩阵具有非常整洁和统一的形式，这也是为什么GLM的拟合算法（如IRLS）和得分检验理论能够适用于所有属于指数分布族的分布（正态、二项、泊松、伽马等）的原因。这个性质是连接不同分布的桥梁，是GLM理论的基石。






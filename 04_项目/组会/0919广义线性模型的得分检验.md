
### 广义线性模型 (GLM) 的得分检验理论

得分检验（Score Test）是一种在**零假设 ($H_0$)** 成立的条件下构建的检验方法。它的核心思想是：**如果零假设是正确的，那么在零假设参数下的最大似然估计处，完整模型的对数似然函数关于“备择假设中新增参数”的梯度（斜率）应该接近于零。**

#### **数学设定**

假设我们的完整模型（Full Model）参数为 $\theta = (\beta, \gamma)$，其中 $\beta$ 是我们感兴趣的参数（例如，新基因型的效应），而 $\gamma$ 是零模型中已有的参数（例如，截距和协变量的效应）。

我们要检验的原假设是 $H_0: \beta = \beta_0$ （通常是 $\beta_0=0$）。

1.  **对数似然函数**: 记为 $\ell(\theta) = \ell(\beta, \gamma)$。
2.  **得分函数 (Score Function)**: 是对数似然函数关于所有参数的**一阶偏导数（梯度）**向量：
    $$
    U(\theta) = \frac{\partial \ell(\theta)}{\partial \theta} = \begin{pmatrix} U_\beta(\theta) \\ U_\gamma(\theta) \end{pmatrix}
    $$
3.  **信息矩阵 (Information Matrix)**: 是得分函数方差-协方差矩阵，等于负的对数似然函数**二阶偏导数（Hessian矩阵）**的期望：
    $$
    I(\theta) = -E\left[\frac{\partial^2 \ell(\theta)}{\partial \theta \partial \theta^T}\right] = \begin{pmatrix} I_{\beta\beta} & I_{\beta\gamma} \\ I_{\gamma\beta} & I_{\gamma\gamma} \end{pmatrix}
    $$

#### **得分检验的推导**

1.  **只拟合零模型**: 在 $H_0: \beta = 0$ 的条件下，我们最大化 $\ell(0, \gamma)$，得到 $\gamma$ 的估计值，记为 $\tilde{\gamma}$。
2.  **评估得分**: 我们在点 $\tilde{\theta} = (0, \tilde{\gamma})$ 处评估**完整模型**的得分函数 $U(\tilde{\theta})$。
    *   根据最大似然估计的性质，由于 $\tilde{\gamma}$ 是零模型的最优解，所以 $U_\gamma(0, \tilde{\gamma}) = 0$。
    *   因此，得分向量在 $\tilde{\theta}$ 处变为：
        $$
        U(\tilde{\theta}) = \begin{pmatrix} U_\beta(0, \tilde{\gamma}) \\ 0 \end{pmatrix}
        $$
        我们只需要关心 $U_\beta(0, \tilde{\gamma})$，这部分通常简记为 $U_\beta$。
3.  **得分统计量**: 在 $H_0$ 下，$U(\tilde{\theta})$ 近似服从均值为0，协方差矩阵为 $I(\tilde{\theta})$ 的正态分布。得分检验统计量 $S$ 是其二次型：
    $$
    S = U(\tilde{\theta})^T [I(\tilde{\theta})]^{-1} U(\tilde{\theta})
    $$
    利用分块矩阵求逆公式，可以证明这个统计量等价于：
    $$
    S = U_\beta^T (I_{\beta\beta} - I_{\beta\gamma}I_{\gamma\gamma}^{-1}I_{\gamma\beta})^{-1} U_\beta
    $$
    其中所有信息矩阵块都在 $\tilde{\theta}$ 处评估。在 $H_0$ 下，$S$ 近似服从自由度为 $\text{dim}(\beta)$ 的卡方分布。

### 广义残差 (r) 和权重矩阵 (W) 的数学原理

得分函数是 **对数似然函数 (log-likelihood) 对我们感兴趣的参数 $\beta$ 的一阶偏导数**。我们用 $l$ 表示总的对数似然函数，它等于每个观测值的对数似然 $l_i$ 之和：
$$
l = \sum_{i=1}^n l_i
$$
因此，得分函数为：
$$
U_\beta = \frac{\partial l}{\partial \beta} = \sum_{i=1}^n \frac{\partial l_i}{\partial \beta}
$$
现在，我们的任务就是计算出 $\frac{\partial l_i}{\partial \beta}$。

### 第一步：运用链式法则

参数 $\beta$ 并非直接出现在 $l_i$ 中。它们之间的关系是这样的：
1.  对数似然 $l_i$ 是关于 $y_i$ 的均值 $\mu_i$ 的函数。
2.  均值 $\mu_i$ 是通过**反向链接函数**与线性预测子 $\eta_i$ 关联的，即 $\mu_i = g^{-1}(\eta_i)$。
3.  线性预测子 $\eta_i$ 是我们参数 $\beta$ 和协变量 $G_i$ 的线性组合，即 $\eta_i = Z_i^T \gamma + G_i^T \beta$。

因此，为了求 $\frac{\partial l_i}{\partial \beta}$，我们必须使用链式法则：

$$
\frac{\partial l_i}{\partial \beta} = \frac{\partial l_i}{\partial \mu_i} \cdot \frac{\partial \mu_i}{\partial \eta_i} \cdot \frac{\partial \eta_i}{\partial \beta}
$$

我们来逐一分解这三个部分。

### 第二步：分解链式法则的三个部分

#### 1. 计算 $\frac{\partial \eta_i}{\partial \beta}$

这是最简单的一部分。根据线性预测子的定义：
$$
\eta_i = Z_i^T \gamma + G_i^T \beta
$$
在零模型 ($H_0$) 下，我们虽然假设 $\beta=0$，但在求导时，我们仍然要对它求偏导。很明显：
$$
\frac{\partial \eta_i}{\partial \beta} = G_i
$$

#### 2. 计算 $\frac{\partial l_i}{\partial \mu_i}$

这一部分是GLM理论中的一个经典结果。对于指数族分布，可以证明一个非常优美的关系：
$$
\frac{\partial l_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}
$$
这个结果来自于对数似然函数对规范参数 $\theta$ 求导，然后再转换到对均值 $\mu$ 求导。它把复杂的求导变成了一个非常直观的形式：**（观测值 - 期望值）/ 方差**。这本质上是一个标准化的残差。

#### 3. $\frac{\partial \mu_i}{\partial \eta_i}$ 这一项

这一项就是反向链接函数 $g^{-1}$ 对线性预测子 $\eta_i$ 的导数。我们暂时就保留这个形式。

### 第三步：组合所有部分

现在，我们将上面得到的三个部分代回到链式法则的公式中：

$$
\frac{\partial l_i}{\partial \beta} = \underbrace{\left( \frac{y_i - \mu_i}{Var(y_i)} \right)}_{\frac{\partial l_i}{\partial \mu_i}} \cdot \underbrace{\left( \frac{\partial \mu_i}{\partial \eta_i} \right)}_{\frac{\partial \mu_i}{\partial \eta_i}} \cdot \underbrace{\left( G_i \right)}_{\frac{\partial \eta_i}{\partial \beta}}
$$

重新整理一下顺序，把 $G_i$ 放在前面：

$$
\frac{\partial l_i}{\partial \beta} = G_i \cdot \left( \frac{y_i - \mu_i}{Var(y_i)} \right) \cdot \left( \frac{\partial \mu_i}{\partial \eta_i} \right)
$$

总的得分函数 $U_\beta$ 是所有观测值的加和：

$$
U_\beta = \sum_{i=1}^n \frac{\partial l_i}{\partial \beta} = \sum_{i=1}^n G_i \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right)
$$

这正是您在问题中给出的第一个公式！到这里，我们已经完成了得分函数的基本推导。

### 第四步：引入广义残差 $r$ 和权重 $W$

现在，我们看看为什么引入 $r_i$ 和 $W_i$ 能让公式更简洁。回顾它们的定义（并且我们所有计算都是在零模型 $\tilde{\mu}_i$ 下进行）：

1.  **权重 $W_i$**: $W_i = \left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2 \frac{1}{Var(y_i)}$
2.  **广义残差 $r_i$**: $r_i = (y_i - \mu_i) \frac{\partial \eta_i}{\partial \mu_i}$

注意，$\frac{\partial \eta_i}{\partial \mu_i} = 1 / \left(\frac{\partial \mu_i}{\partial \eta_i}\right)$。所以 $r_i$ 也可以写成：
$$
r_i = \frac{y_i - \mu_i}{\partial \mu_i / \partial \eta_i}
$$

让我们看看如何从 $U_\beta$ 的表达式中凑出这两个量。

我们从上一步得到的 $U_\beta$ 表达式开始：
$$
U_\beta = \sum_{i=1}^n G_i \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right)
$$

现在，我们对括号里的部分做一个巧妙的代数变换：**分子分母同时乘以 $\frac{\partial \mu_i}{\partial \eta_i}$**。

$$
U_\beta = \sum_{i=1}^n G_i \left( \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)} \cdot \frac{y_i - \mu_i}{\frac{\partial \mu_i}{\partial \eta_i}} \right)
$$

仔细观察这个新形式，我们发现：
*   第一部分 $\frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}$ 正是**权重 $W_i$** 的定义！
*   第二部分 $\frac{y_i - \mu_i}{\frac{\partial \mu_i}{\partial \eta_i}}$ 正是**广义残差 $r_i$** 的定义！

所以，我们可以把 $U_\beta$ 写成：
$$
U_\beta = \sum_{i=1}^n G_i \cdot W_i \cdot r_i
$$

如果我们将 $G$ 看作是 $n \times q$ 的设计矩阵（$q$ 是 $\beta$ 的维度），$W$ 是一个对角线上为 $W_i$ 的 $n \times n$ 对角矩阵，$r$ 是一个包含 $r_i$ 的 $n \times 1$ 的向量，那么上面的求和可以写成矩阵形式：

$$
U_\beta = G^T W r
$$

### 总结与解释

1.  **得分函数 $U_\beta$ 的本质**：它衡量了在零模型（即不包含 $G$ 的模型）下，**新协变量 $G$ 与模型残差之间的“相关性”**。如果 $G$ 与残差高度相关，说明 $G$ 可以解释当前模型无法解释的变异，那么 $G$ 应该被加入模型，$\beta$ 不为零。

2.  **为什么是“广义”残差 $r$**：它不是简单的 $y_i - \mu_i$。它还除以了 $\frac{\partial \mu_i}{\partial \eta_i}$。这一步的作用是 **将残差从原始响应变量 $y$ 的尺度，转换到线性预测子 $\eta$ 的尺度**。这使得不同链接函数下的残差具有可比性，所以它被称为“工作因变量”或“广义残差”。

3.  **为什么需要权重 $W$**：权重 $W_i$ 同时考虑了两个因素：
    *   **方差 $Var(y_i)$**：方差大的观测值（信息量少，不稳定）应该被赋予较小的权重。
    *   **链接函数的影响 $(\frac{\partial \mu_i}{\partial \eta_i})^2$**：这个导数衡量了线性预测子 $\eta_i$ 的微小变化能引起均值 $\mu_i$ 多大的变化。在某些区域（例如 logistic 回归中概率接近 0 或 1 的地方），这个导数很小，说明模型在该处非常“不敏感”，因此这些观测值也应被赋予较小的权重。

最终得到的 $U_\beta = G^T W r$ 是一个非常优雅和强大的形式。它告诉我们，**得分统计量本质上是新协变量 $G$ 与经过方差和链接函数校正后的广义残差 $r$ 之间的加权和（或加权相关性）**。

您代码中计算的 `residuals`，如果最终用于构建得分检验，那么它很可能就是这个 $r_i$ (广义残差) 或者 $W_i r_i$ (加权广义残差) 的具体实现。




## 信息矩阵

我们的目标是计算这个 Hessian 矩阵，然后取其负期望，最后将结果表示成 $X^T W X$ 的形式。

### 第一步：回顾一阶导数（得分函数）

在之前的推导中，我们得到了单个观测值 $l_i$ 对参数 $\theta$ 的一阶导数：
$$
\frac{\partial l_i}{\partial \theta} = \frac{\partial \eta_i}{\partial \theta} \cdot \frac{\partial \mu_i}{\partial \eta_i} \cdot \frac{\partial l_i}{\partial \mu_i}
$$
我们知道 $\frac{\partial \eta_i}{\partial \theta}$ 是设计矩阵的第 $i$ 行 $X_i^T$，并且 $\frac{\partial l_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}$。所以：
$$
\frac{\partial l_i}{\partial \theta} = X_i^T \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right)
$$
总的得分函数是 $U = \sum_{i=1}^n \frac{\partial l_i}{\partial \theta}$。

### 第二步：计算二阶导数（Hessian 矩阵）

现在，我们要对上面的得分函数再求一次导数，得到 Hessian 矩阵 $H = \frac{\partial^2 l}{\partial \theta \partial \theta^T}$。
$$
H = \frac{\partial U}{\partial \theta^T} = \sum_{i=1}^n \frac{\partial}{\partial \theta^T} \left[ X_i^T \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right) \right]
$$
这是一个 $p \times p$ 的矩阵（其中 $p$ 是参数总数）。我们可以把它写成 $\sum X_i^T (\dots) X_i$ 的形式。我们用链式法则来处理括号里的部分，注意这里是对行向量 $\theta^T$ 求导，实际上等价于对 $\eta_i$ 求导再乘以 $\frac{\partial \eta_i}{\partial \theta^T} = X_i$：
$$
H = \sum_{i=1}^n X_i^T \left[ \frac{\partial}{\partial \eta_i} \left( \frac{\partial \mu_i}{\partial \eta_i} \frac{y_i - \mu_i}{Var(y_i)} \right) \right] X_i
$$
现在我们来处理中间的导数项，这需要用到乘法法则：
$$
\frac{\partial}{\partial \eta_i} \left( \dots \right) = \underbrace{\frac{\partial}{\partial \eta_i} \left(\frac{\partial \mu_i}{\partial \eta_i} \frac{1}{Var(y_i)}\right) \cdot (y_i - \mu_i)}_{\text{Term A}} + \underbrace{\left(\frac{\partial \mu_i}{\partial \eta_i} \frac{1}{Var(y_i)}\right) \cdot \frac{\partial (y_i - \mu_i)}{\partial \eta_i}}_{\text{Term B}}
$$
-   **Term A**: 这是一个很复杂的项，它依赖于链接函数和方差函数对 $\eta_i$ 的二阶导数。
-   **Term B**: 这一项比较简单。因为 $\frac{\partial y_i}{\partial \eta_i} = 0$（$y_i$ 是常数），所以 $\frac{\partial (y_i - \mu_i)}{\partial \eta_i} = - \frac{\partial \mu_i}{\partial \eta_i}$。
    因此，Term B = $-\left(\frac{\partial \mu_i}{\partial \eta_i} \frac{1}{Var(y_i)}\right) \cdot \frac{\partial \mu_i}{\partial \eta_i} = - \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}$。

把它们代回去，Hessian 矩阵就是：
$$
H = \sum_{i=1}^n X_i^T \left[ \text{Term A} \cdot (y_i - \mu_i) - \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)} \right] X_i
$$

### 第三步：取负期望值（关键的简化步骤）

现在我们要计算信息矩阵 $I = -E[H]$。期望是作用在随机变量 $y_i$ 上的。
$$
I = -E \left[ \sum_{i=1}^n X_i^T \left[ \text{Term A} \cdot (y_i - \mu_i) - \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)} \right] X_i \right]
$$
根据期望的线性性质，我们可以把求和与期望交换顺序：
$$
I = \sum_{i=1}^n X_i^T \left[ -E[\text{Term A} \cdot (y_i - \mu_i)] + E\left[\frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}\right] \right] X_i
$$
这里就是奇迹发生的地方：
1.  **对于第一部分**：$E[y_i - \mu_i] = E[y_i] - \mu_i = \mu_i - \mu_i = 0$。因为 Term A 部分不依赖于随机变量 $y_i$，所以 $E[\text{Term A} \cdot (y_i - \mu_i)] = \text{Term A} \cdot E[y_i - \mu_i] = 0$。
    **这个复杂的 Term A 在取期望后就消失了！** 这就是为什么我们使用期望信息矩阵（Fisher Information）而不是观测信息矩阵（Observed Information，即Hessian本身）的原因之一，它极大地简化了计算。

2.  **对于第二部分**：$\frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}$ 这个表达式中所有项都只依赖于均值 $\mu_i$（方差通常是均值的函数），而不依赖于随机的 $y_i$。因此，它对于期望算子来说是常数。
    $E\left[\frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}\right] = \frac{(\frac{\partial \mu_i}{\partial \eta_i})^2}{Var(y_i)}$。

这正是我们之前定义的**权重 $W_i$**！
$$
W_i = \left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2 \frac{1}{Var(y_i)}
$$

所以，整个表达式简化为：
$$
I = \sum_{i=1}^n X_i^T W_i X_i
$$

### 第四步：写成矩阵形式并分块

这个求和 $\sum_{i=1}^n X_i^T W_i X_i$ 正是矩阵乘法 **$X^T W X$** 的定义，其中 $X$ 是完整的 $n \times p$ 设计矩阵，$W$ 是一个 $n \times n$ 的对角矩阵，对角线上的元素是 $W_1, W_2, \dots, W_n$。

所以我们得到了核心结果：
$$
I \approx X^T W X
$$
（这里的“约等于”表示我们用期望信息矩阵近似了观测信息矩阵）

现在，我们将完整的设计矩阵 $X$ 和参数 $\theta$ 分块：
$$
X = [Z, G] \quad \text{和} \quad \theta = \begin{pmatrix} \gamma \\ \beta \end{pmatrix}
$$
信息矩阵 $I$ 也相应地分块：
$$
I = \begin{pmatrix} I_{\gamma\gamma} & I_{\gamma\beta} \\ I_{\beta\gamma} & I_{\beta\beta} \end{pmatrix}
$$
我们将分块的 $X$ 代入 $I = X^T W X$：
$$
I = \begin{pmatrix} Z^T \\ G^T \end{pmatrix} W \begin{pmatrix} Z & G \end{pmatrix}
$$
执行这个分块矩阵乘法：
$$
I = \begin{pmatrix}
Z^T W Z & Z^T W G \\
G^T W Z & G^T W G
\end{pmatrix}
$$
通过直接比较两个分块矩阵，我们就得到了您问题中的结论：
$$
I_{\gamma\gamma} \approx Z^T W Z
$$
$$
I_{\beta\beta} \approx G^T W G
$$
$$
I_{\beta\gamma} \approx G^T W Z
$$

### 结论与直觉

这个结果非常漂亮，它告诉我们，在GLM框架下，信息矩阵（它决定了参数估计的方差）可以被看作是**设计矩阵的加权交叉积**，形式上与普通最小二乘法中的 $(X^T X)$ 非常相似。

-   在OLS中，信息矩阵（的某个缩放）是 $X^T X$。
-   在GLM中，它是 $X^T W X$。

**权重矩阵 $W$** 起到了关键的调节作用，它为每个观测值赋予了不同的“信息量”。一个观测值的权重 $W_i$ 大，意味着：
1.  它的**方差 $Var(y_i)$ 比较小**（数据点更稳定，信息量更大）。
2.  它所处的位置让**均值 $\mu_i$ 对线性预测子 $\eta_i$ 的变化更敏感**（即 $\frac{\partial \mu_i}{\partial \eta_i}$ 比较大）。

这个 $X^T W X$ 矩阵不仅是理论上的信息矩阵，它也是**拟合GLM的“迭代重加权最小二乘法”（IRLS）算法**中每一步迭代的核心。算法在每一步都会根据当前的参数估计更新权重 $W$，然后求解一个加权最小二乘问题来更新参数。







## step-by-step derivation of the score test for the **proportional odds ordinal logistic regression model**.

### Step 0: The Model and Likelihood

First, let's precisely define the model. Let $Y_i$ be the ordinal response for individual $i$, taking one of $K$ ordered categories, $\{1, 2, \dots, K\}$.

1.  **Cumulative Probabilities**: The model doesn't directly model $P(Y_i=k)$, but rather the cumulative probability $\pi_{ij} = P(Y_i \le j)$ for $j=1, \dots, K-1$.

2.  **Link Function**: The link function is the logit function.

3.  **Linear Predictor**: The model assumes a **single** linear predictor $\eta_i$ that is common across all cumulative probabilities. This is the "proportional odds" assumption.
    $$ \eta_i = Z_i^T\gamma + G_i^T\beta $$

4.  **Model Equation**: The relationship between these parts is:
    $$ \text{logit}(\pi_{ij}) = \alpha_j - \eta_i $$
    Here, $\alpha_j$ are the category-specific intercepts or thresholds, which must be ordered ($\alpha_1 < \alpha_2 < \dots < \alpha_{K-1}$). The negative sign on $\eta_i$ is a common convention that makes interpretation easier: a positive $\beta$ means higher values of $G_i$ are associated with higher categories of $Y_i$.

5.  **Parameters**: The full parameter vector is $\theta = (\beta^T, \gamma^T, \alpha_1, \dots, \alpha_{K-1})^T$.
    Under the null hypothesis $H_0: \beta=0$, the nuisance parameters are $\psi = (\gamma^T, \alpha^T)^T$.

6.  **Log-Likelihood**: The model is based on a multinomial distribution. The probability of observing category $k$ is $p_{ik} = P(Y_i=k) = \pi_{ik} - \pi_{i,k-1}$ (with $\pi_{i0}=0$ and $\pi_{iK}=1$).
    Let's use indicator variables $d_{ik} = I(y_i=k)$ where $y_i$ is the observed category for individual $i$. The log-likelihood for a single observation is:
    $$ l_i = \sum_{k=1}^K d_{ik} \log(p_{ik}) $$
    The total log-likelihood is $l = \sum_{i=1}^n l_i$.

---

### Step 1: The Score Function $U_\beta$

Our goal is to compute $U_\beta = \frac{\partial l}{\partial \beta}$ and evaluate it under the null model (where $\beta=0$ and $\gamma, \alpha$ are estimated as $\tilde{\gamma}, \tilde{\alpha}$).

We use the chain rule, but it's more complex now because $\beta$ (via $\eta_i$) affects *all* $\pi_{ij}$ simultaneously.

$$
\frac{\partial l_i}{\partial \beta} = \frac{\partial l_i}{\partial \eta_i} \cdot \frac{\partial \eta_i}{\partial \beta}
$$

1.  **Calculate $\frac{\partial \eta_i}{\partial \beta}$**: This is the easy part, just as in GLM.
    $$ \frac{\partial \eta_i}{\partial \beta} = G_i $$

2.  **Calculate $\frac{\partial l_i}{\partial \eta_i}$**: This is the core of the problem. We need to trace the effect of $\eta_i$ on $l_i$ through all the cumulative probabilities.
    $$ \frac{\partial l_i}{\partial \eta_i} = \sum_{j=1}^{K-1} \frac{\partial l_i}{\partial \pi_{ij}} \cdot \frac{\partial \pi_{ij}}{\partial \eta_i} $$
    *   **Term 1: $\frac{\partial \pi_{ij}}{\partial \eta_i}$**: Since $\pi_{ij} = \text{logit}^{-1}(\alpha_j - \eta_i)$, and the derivative of the logistic function $\sigma(x)$ is $\sigma(x)(1-\sigma(x))$, we get:
        $$ \frac{\partial \pi_{ij}}{\partial \eta_i} = \pi_{ij}(1-\pi_{ij}) \cdot \frac{\partial(\alpha_j - \eta_i)}{\partial \eta_i} = -\pi_{ij}(1-\pi_{ij}) $$
    *   **Term 2: $\frac{\partial l_i}{\partial \pi_{ij}}$**: This is the trickiest part. The log-likelihood $l_i$ depends on $p_{ik}$, and each $\pi_{ij}$ affects two probabilities: $p_{ij} = \pi_{ij} - \pi_{i,j-1}$ and $p_{i,j+1} = \pi_{i,j+1} - \pi_{ij}$. Using the chain rule for derivatives:
        $$ \frac{\partial l_i}{\partial \pi_{ij}} = \frac{d_{ij}}{p_{ij}}\frac{\partial p_{ij}}{\partial \pi_{ij}} + \frac{d_{i,j+1}}{p_{i,j+1}}\frac{\partial p_{i,j+1}}{\partial \pi_{ij}} = \frac{d_{ij}}{p_{ij}} - \frac{d_{i,j+1}}{p_{i,j+1}} $$

    Combining these gives a complicated-looking expression for $\frac{\partial l_i}{\partial \eta_i}$. However, there is a more elegant and well-known result for the score of the proportional odds model:
    $$ \frac{\partial l_i}{\partial \eta_i} = y_i - E[Y_i] $$
    ... where the expectation is taken with respect to the $K$ categories. This is true for some specific parameterizations but not the most general. A more direct and always correct (though less intuitive) representation comes from summing up the effects:
    The final expression for the score of observation $i$ w.r.t $\eta_i$ can be shown to simplify to:
    $$
    \frac{\partial l_i}{\partial \eta_i} = \sum_{j=1}^{K-1} \left( \pi_{ij} - I(y_i \le j) \right) = \sum_{j=1}^{K-1} (\pi_{ij} - D_{ij})
    $$
    where $D_{ij} = I(y_i \le j)$ is the observed cumulative indicator. This is a beautiful result! It states that the derivative w.r.t. the linear predictor is the sum of the "errors" across all possible cumulative cutoffs.

3.  **Combine to get $U_\beta$**:
    $$
    U_\beta = \sum_{i=1}^n \frac{\partial l_i}{\partial \beta} = \sum_{i=1}^n G_i \cdot \frac{\partial l_i}{\partial \eta_i} = \sum_{i=1}^n G_i \left( \sum_{j=1}^{K-1} (\tilde{\pi}_{ij} - D_{ij}) \right)
    $$
    All probabilities $\tilde{\pi}_{ij}$ are calculated using the null model estimates $(\tilde{\gamma}, \tilde{\alpha})$. This is the final form of the score function. It still represents the covariance between the new predictor $G$ and the model's residual error, but the error is now a sum over all categories.

---

### Step 2: The Information Matrix

This is where the difference from GLM becomes most apparent. The nuisance parameters $\psi$ now include both $\gamma$ and the $K-1$ intercepts $\alpha$. The information matrix $I(\theta)$ must be partitioned accordingly.

The key insight, analogous to the GLM derivation, is that the information matrix blocks can be expressed using a **weight matrix**. However, this weight is no longer a scalar $W_i$ for each observation.

1.  **From Scalar to Matrix Weights**: For GLM, the weight $W_i$ was related to the variance of a single response $y_i$. For the ordinal model, the "response" for observation $i$ is effectively the vector of $K-1$ indicators $(D_{i1}, D_{i2}, \dots, D_{i,K-1})$. Therefore, the weight for observation $i$ becomes a **$(K-1) \times (K-1)$ covariance matrix, $W_i$**.

2.  **Defining the Weight Matrix $W_i$**: The entry $(j,k)$ of $W_i$ is the covariance between the indicators $D_{ij}$ and $D_{ik}$ under the model.
    $$
    [W_i]_{jk} = \text{Cov}(D_{ij}, D_{ik}) = E[D_{ij}D_{ik}] - E[D_{ij}]E[D_{ik}]
    $$
    Assuming $j \le k$:
    *   $E[D_{ij}] = P(Y_i \le j) = \pi_{ij}$
    *   $E[D_{ik}] = P(Y_i \le k) = \pi_{ik}$
    *   $E[D_{ij}D_{ik}] = E[I(y_i \le j) \cdot I(y_i \le k)] = E[I(y_i \le j)] = \pi_{ij}$
    Therefore, for $j \le k$:
    $$
    [W_i]_{jk} = \pi_{ij} - \pi_{ij}\pi_{ik} = \pi_{ij}(1-\pi_{ik})
    $$
    So, the weight for *each observation* is a full covariance matrix reflecting the relationships between the different category cutoffs.

3.  **Information Matrix Blocks**: The information matrix no longer has a simple $X^T WX$ form because of the separate structure of the intercepts $\alpha_j$. A full derivation is extremely lengthy, but the conceptual result is what matters for the test statistic. The blocks of the information matrix, $I_{\beta\beta}$, $I_{\beta\psi}$, and $I_{\psi\psi}$, are sums over all individuals of terms involving the design matrices ($G_i$, $Z_i$, and vectors of 1s for the intercepts) and these weight matrices $W_i$.

---

### Step 3: The Score Test Statistic

The general form of the statistic is unchanged:
$$
S = U_\beta^T (I_{\beta\beta} - I_{\beta\psi}I_{\psi\psi}^{-1}I_{\psi\beta})^{-1} U_\beta
$$
All components are evaluated at the null model estimates.

The term $(I_{\beta\beta} - I_{\beta\psi}I_{\psi\psi}^{-1}I_{\psi\beta})$ is the **variance of the score $U_\beta$**, adjusted for the fact that the nuisance parameters $\psi = (\gamma, \alpha)$ had to be estimated.

**Intuitive Interpretation (Connection to Weighted Least Squares)**

While the formulas are complex, the final statistic has a powerful and familiar interpretation. It is equivalent to a test on the coefficients of $G$ in a specific weighted least squares regression:
1.  Define a "working residual" for each observation, which is related to the score contribution from that observation.
2.  Define a "weight matrix" for each observation, which is related to the information contribution from that observation (our $W_i$ matrix).
3.  The term $I_{\beta\psi}I_{\psi\psi}^{-1}I_{\psi\beta}$ represents the "projection" of the information about $\beta$ onto the space spanned by the nuisance parameters $\gamma$ and $\alpha$.
4.  The final statistic $S$ essentially measures the squared length of the score vector $U_\beta$ after it has been projected onto the space that is **orthogonal** to the nuisance parameter scores. In simpler terms, it measures **how much "signal" for $\beta$ is left in the residuals after we've already explained everything we can with the null model variables ($Z$) and the baseline category structure ($\alpha$)**.

Under $H_0$, $S$ follows a chi-squared distribution with degrees of freedom equal to the number of parameters in $\beta$.








### $\frac{\partial \ell_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}$的推导

这个结论来源于GLM所基于的**指数分布族（Exponential Dispersion Family）**的数学形式。

### 1. 指数分布族的标准形式

一个概率分布如果属于指数分布族，其概率密度函数（或概率质量函数）$f(y_i; \theta_i, \phi)$ 可以写成以下标准形式：

$$
f(y_i; \theta_i, \phi) = \exp\left[ \frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right]
$$

这里：
*   $y_i$: 观测值。
*   $\theta_i$: **自然参数 (natural parameter)**。这是模型的“核心”参数，它直接与线性预测器 $\eta_i$ 相关（通常 $\eta_i = \theta_i$）。
*   $\phi$: **离散参数 (dispersion parameter)**。它与方差有关，对于某些分布（如二项分布和泊松分布），它是一个固定的常数（通常 $a(\phi)=1$）。
*   $a(\cdot), b(\cdot), c(\cdot)$: 是已知的函数，它们定义了具体的分布是哪一个（如正态分布、泊松分布等）。

**对数似然函数**就是对上面这个式子取对数：
$$
\ell_i = \log f(y_i; \theta_i, \phi) = \frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi)
$$

### 2. 指数分布族的重要性质

指数分布族有两个非常优美的数学性质，它们是通过对 $b(\theta_i)$ 函数求导得到的：

*   **性质1：均值 (Expectation)**
    $$
    E(y_i) = \mu_i = b'(\theta_i) = \frac{d b(\theta_i)}{d \theta_i}
    $$
    也就是说，响应变量的期望值 $\mu_i$ 是函数 $b(\cdot)$ 对自然参数 $\theta_i$ 的一阶导数。

*   **性质2：方差 (Variance)**
    $$
    Var(y_i) = b''(\theta_i) a(\phi) = \frac{d^2 b(\theta_i)}{d \theta_i^2} a(\phi)
    $$
    响应变量的方差是 $b(\cdot)$ 的二阶导数乘以离散参数函数 $a(\phi)$。

### 3. 推导 $\frac{\partial \ell_i}{\partial \mu_i}$

现在，我们有了所有需要的工具。我们要计算的是对数似然 $\ell_i$ 对**均值 $\mu_i$** 的偏导数。这是一个链式法则问题：

$$
\frac{\partial \ell_i}{\partial \mu_i} = \frac{\partial \ell_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i}
$$

我们来分别计算这两部分：

#### **第一部分: $\frac{\partial \ell_i}{\partial \theta_i}$ (对自然参数求导)**

我们对对数似然函数 $\ell_i$ 直接关于 $\theta_i$ 求导：
$$
\frac{\partial \ell_i}{\partial \theta_i} = \frac{\partial}{\partial \theta_i} \left[ \frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right]
$$
因为 $y_i$ 和 $\phi$ 被视为常数，所以 $c(y_i, \phi)$ 的导数为0。我们得到：
$$
\frac{\partial \ell_i}{\partial \theta_i} = \frac{y_i - b'(\theta_i)}{a(\phi)}
$$
现在，利用**性质1** ($b'(\theta_i) = \mu_i$)，我们可以把上式替换为：
$$
\frac{\partial \ell_i}{\partial \theta_i} = \frac{y_i - \mu_i}{a(\phi)}
$$

#### **第二部分: $\frac{\partial \theta_i}{\partial \mu_i}$ (自然参数对均值求导)**

这部分是第一部分的逆运算。我们知道 $b'(\theta_i) = \mu_i$。根据反函数求导法则，我们先求 $\frac{\partial \mu_i}{\partial \theta_i}$：
$$
\frac{\partial \mu_i}{\partial \theta_i} = \frac{\partial}{\partial \theta_i} (b'(\theta_i)) = b''(\theta_i)
$$
所以，我们要求的导数就是它的倒数：
$$
\frac{\partial \theta_i}{\partial \mu_i} = \frac{1}{b''(\theta_i)}
$$

#### **组合两部分**

现在，我们将两部分乘起来：
$$
\frac{\partial \ell_i}{\partial \mu_i} = \frac{\partial \ell_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i} = \left( \frac{y_i - \mu_i}{a(\phi)} \right) \cdot \left( \frac{1}{b''(\theta_i)} \right) = \frac{y_i - \mu_i}{b''(\theta_i) a(\phi)}
$$
最后，我们观察分母 $b''(\theta_i) a(\phi)$。根据**性质2**，这正是 $Var(y_i)$！

所以，我们最终得到了这个优美的结论：
$$
\frac{\partial \ell_i}{\partial \mu_i} = \frac{y_i - \mu_i}{Var(y_i)}
$$

### 结论

这个性质是指数分布族与生俱来的内在数学结构所决定的。它之所以重要，是因为它揭示了一个深刻的联系：

**对数似然函数关于均值的梯度，正比于“残差”($y_i - \mu_i$)，并由方差进行了标准化。**

这使得GLM的得分函数和信息矩阵具有非常整洁和统一的形式，这也是为什么GLM的拟合算法（如IRLS）和得分检验理论能够适用于所有属于指数分布族的分布（正态、二项、泊松、伽马等）的原因。这个性质是连接不同分布的桥梁，是GLM理论的基石。







---
## **如何将有序分类表型转换为可用于STAAR分析的数值型数据？**

**核心方法：潜在变量残差法 (The Latent Residual Method)**

### 理论基础：有序数据的潜在变量模型

该方法始于一个有序回归模型，其背后有一个非常直观的假设：

1.  **存在一个未被观测到的、连续的潜在变量 Y***。这个Y*代表了个体真实的、潜在的性状水平（例如，真实的“饮酒倾向”或“疾病严重程度”）。

2.  这个潜在变量 Y* 服从一个标准的线性模型：
    `Y* = β₀ + β₁X₁ + ... + βₚXₚ + ε`
    其中，`X`是协变量（如年龄、性别、PCs、PRS等），`β`是它们的效应系数，`ε`是随机误差。

3.  我们观测到的有序分类 `Y` (例如，饮酒频率等级 1, 2, ..., 6) 是由 `Y*` 和一系列阈值（Thresholds, α）共同决定的。
    *   如果 `Y* ≤ α₁`，我们观测到 `Y = 1`
    *   如果 `α₁ < Y* ≤ α₂`，我们观测到 `Y = 2`
    *   ...

*   **`link = "probit"` 的关键作用**: 这个参数在数学上等同于假设模型的误差项 `ε` 服从**标准正态分布 N(0, 1)**。
* 参考：
* Alan Agresti 《Analysis of Ordinal Categorical Data》
* Peter McCullagh 《Regression Models for Ordinal Data》
好的，我们来使用 LaTeX 将这段核心思想和计算步骤的描述变得更加清晰和专业。

---

### 核心思想与计算步骤

- 既然我们无法直接观测到潜变量 $Y^*$，那么我们也无法观测到真实的残差 $\epsilon$。
- 但是，利用拟合好的有序probit模型，我们可以为每个个体计算出其残差 $\epsilon$ 的**条件期望值** $E[\epsilon | \mathbf{X}, Y]$。
- 这个条件期望是我们对真实残差的最佳估计，并可直接作为后续分析的数值型表型。

**计算步骤**：

1.  **拟合模型**：
    首先，我们使用 `ordinal::clm` 拟合零模型，得到协变量的效应向量 $\hat{\beta}$ 和阈值向量 $\hat{\alpha} = (\hat{\alpha}_1, \dots, \hat{\alpha}_{K-1})$。

2.  **确定残差的“截尾”范围**:
    对于一个特定的个体 $i$，我们已知其协变量向量 $\mathbf{X}_i$ 和他最终被观察到的表型分类 $Y_i=k$。
    -   模型的线性预测部分为：
        $$ \eta_i = \mathbf{X}_i \hat{\beta} $$
    -   根据 $Y_i=k$，我们知道他的潜在变量 $Y_i^*$ 落在区间 $(\hat{\alpha}_{k-1}, \hat{\alpha}_k]$ 内（我们定义 $\hat{\alpha}_0 = -\infty$ 且 $\hat{\alpha}_K = +\infty$）。
    -   由于 $\epsilon_i = Y_i^* - \eta_i$，我们同样知道了他的**真实残差 $\epsilon_i$ 必须落在**如下区间内：
        $$ (\hat{\alpha}_{k-1} - \eta_i, \quad \hat{\alpha}_k - \eta_i] $$
        我们令区间的下界为 $a_i = \hat{\alpha}_{k-1} - \eta_i$，上界为 $b_i = \hat{\alpha}_k - \eta_i$。

3.  **计算条件期望（截尾正态分布的期望）**:
    现在的问题变成了：已知一个服从标准正态分布的随机变量 $\epsilon \sim N(0, 1)$，当它被截尾在区间 $(a_i, b_i]$ 内时，它的期望值是多少？
    -   这个条件期望有一个精确的解析解：
        $$ E[\epsilon_i | a_i < \epsilon_i \le b_i] = \frac{\phi(a_i) - \phi(b_i)}{\Phi(b_i) - \Phi(a_i)} $$
        其中，$\phi(\cdot)$ 是标准正态分布的概率密度函数 (PDF)，$\Phi(\cdot)$ 是其累积分布函数 (CDF)。
    -   在代码中，`residuals <- (dnorm(lower_b) - dnorm(upper_b)) / prob_interval` 这行正是在执行这个计算。

4.  **生成新表型（即残差）**:
    这个计算出的**条件期望残差** $E[\epsilon_i | \mathbf{X}_i, Y_i]$ 就被直接用作后续分析的数值。由于它的总体均值理论上为0，它本身就是一个可以直接用于分数检验的残差向量。
    

---

### 数学证明：条件期望 $E[Y | X]$ 是在均方误差最小化准则下的最优预测

为了使证明更具一般性，我们使用通用的随机变量 $Y$ 和 $X$，其中：

-   $Y$: 我们想要预测的目标随机变量（在您的情况下，是不可观测的残差 $\epsilon$）。
-   $X$: 我们已知的、用于预测的信息（在您的情况下，是协变量 $\mathbf{X}$ 和观测类别 $Y_{obs}$）。
-   $g(X)$: 任何一个基于信息 $X$ 构建的预测函数。我们的目标是找到一个最优的 $g(X)$。

### 目标：最小化均方误差 (MSE)

我们的目标是找到一个预测函数 $g(X)$，使得预测值 $g(X)$ 与真实值 $Y$ 之间的**均方误差**最小。均方误差定义为：

$$ \text{MSE} = E\left[ (Y - g(X))^2 \right] $$

我们想要求解：

$$ \arg\min_{g(X)} E\left[ (Y - g(X))^2 \right] $$

### 证明过程

这个证明过程利用了期望的线性和迭代期望定律（Law of Iterated Expectations）。

1.  **展开平方项**:
    我们先对 $(Y - g(X))^2$ 进行代数变换，通过在中间加上和减去 $E[Y | X]$ 这一项：
    $$ E\left[ (Y - g(X))^2 \right] = E\left[ \left( (Y - E[Y | X]) + (E[Y | X] - g(X)) \right)^2 \right] $$

2.  **应用 $(a+b)^2 = a^2 + 2ab + b^2$**:
    令 $a = Y - E[Y | X]$ 且 $b = E[Y | X] - g(X)$。展开后得到：
    $$ = E\left[ (Y - E[Y | X])^2 + 2(Y - E[Y | X])(E[Y | X] - g(X)) + (E[Y | X] - g(X))^2 \right] $$

3.  **利用期望的线性性质 $E[A+B] = E[A] + E[B]$**:
    我们将期望算子分配到三项中：
    $$ = E\left[ (Y - E[Y | X])^2 \right] + E\left[ 2(Y - E[Y | X])(E[Y | X] - g(X)) \right] + E\left[ (E[Y | X] - g(X))^2 \right] $$

4.  **聚焦于中间的交叉项**:
    我们来处理交叉项 $E\left[ 2(Y - E[Y | X])(E[Y | X] - g(X)) \right]$。
    首先，常数 2 可以提出。其次，根据**迭代期望定律** $E[A] = E[E[A | X]]$，我们可以写出：
    $$ E\left[ (Y - E[Y | X])(E[Y | X] - g(X)) \right] = E\left[ E\left[ (Y - E[Y | X])(E[Y | X] - g(X)) \Big| X \right] \right] $$

5.  **在条件期望内部进行操作**:
    在内部的条件期望 $E[\cdot | X]$ 中，项 $(E[Y | X] - g(X))$ 是一个只依赖于 $X$ 的函数，因此在给定 $X$ 的条件下，它是一个常量，可以提到期望外面：
    $$ = E\left[ (E[Y | X] - g(X)) \cdot E\left[ (Y - E[Y | X]) \Big| X \right] \right] $$

6.  **关键一步：交叉项为零**:
    我们来计算 $E\left[ (Y - E[Y | X]) \Big| X \right]$：
    $$ E\left[ (Y - E[Y | X]) \Big| X \right] = E[Y | X] - E[ E[Y | X] \Big| X ] $$
    由于 $E[Y | X]$ 本身就是 $X$ 的一个函数，所以对它再次取关于 $X$ 的条件期望，结果还是它自己。即 $E[ E[Y | X] | X ] = E[Y | X]$。
    因此：
    $$ E\left[ (Y - E[Y | X]) \Big| X \right] = E[Y | X] - E[Y | X] = 0 $$
    这意味着整个交叉项都变成了**零**！

7.  **回到主公式**:
    现在，原来的均方误差公式简化为：
    $$ \text{MSE} = E\left[ (Y - g(X))^2 \right] = E\left[ (Y - E[Y | X])^2 \right] + E\left[ (E[Y | X] - g(X))^2 \right] $$

8.  **分析简化后的公式**:
    *   **第一项 $E\left[ (Y - E[Y | X])^2 \right]$**: 这是 $Y$ 的**条件方差 $\text{Var}(Y | X)$**。它代表了在已知 $X$ 的情况下，$Y$ 仍然存在的、**不可避免的随机性**。这一项的值是固定的，**与我们选择的预测函数 $g(X)$ 无关**。它是我们预测误差的理论下限。
    *   **第二项 $E\left[ (E[Y | X] - g(X))^2 \right]$**: 这一项衡量了我们的预测函数 $g(X)$ 与“理想”预测 $E[Y | X]$ 之间的均方差异。这一项**只与我们选择的 $g(X)$ 有关**。

9.  **得出结论**:
    为了最小化整个 $\text{MSE}$，我们能做的唯一事情就是让第二项 $E\left[ (E[Y | X] - g(X))^2 \right]$ 尽可能小。由于平方项 $( \cdot )^2$ 永远是非负的，所以它的期望也永远是非负的。
    
    **它的最小值是零。**
    
    这个最小值在什么时候达到呢？当且仅当 $(E[Y | X] - g(X))^2 = 0$ 对几乎所有 $X$ 都成立时，也就是：
    
    $$ g(X) = E[Y | X] $$

太完美了！这份JSON数据就是我们一直在寻找的“罗塞塔石碑”，它清晰地揭示了网站API之间的关联方式。

根据这份JSON，我们可以精确地更新我们的爬虫代码，让它能够真正地自动化整个查询流程。

---

### 从JSON数据中获得的关键情报

1.  **顶层结构**: JSON的最外层是 `{"data": {"query": ..., "results": ...}}`。我们的数据在 `data['results']` 里面。

2.  **`results` 结构**: `results` 是一个字典，包含了三个键：`"genes"`, `"phenotypes"`, `"variants"`。
    *   在我们搜索 "Septi" 的例子中，`"genes"` 和 `"variants"` 都是空列表 `[]`。
    *   所有返回的结果都在 `"phenotypes"` 这个键对应的列表中。

3.  **`phenotypes` 列表**:
    *   这是一个包含多个字典的列表，每个字典代表一个匹配到的表型。
    *   每个表型字典里，最重要的键是：
        *   **`"id"`**: 这就是我们苦苦寻找的 **Phenotype ID** (例如 `"cb38372f-71a1-4daf-8b77-a81f11b9d5c5"`)。
        *   **`"name"`**: 表型的可读名称 (例如 `"Union#R572#Septic shock"`)。
        *   **`"datasets"`**: 这是一个列表，包含了这个表型存在于哪些数据集中。

4.  **`datasets` 列表 (嵌套在phenotype里)**:
    *   每个数据集字典里，最重要的键是：
        *   **`"dataset_id"`**: 原始数据集的ID。
        *   **`"version_id"`**: **这很可能就是我们需要的 `dataset_id`**，用于构建最终的`associations` API URL (例如 `"6319c068-fd59-46d8-85ee-82d82482eb14"`)。
        *   **`"dataset_name"`**: 数据集的可读名称 (例如 `"UK Biobank 500K"`)。

### 修正和完善爬虫逻辑

现在，我们的爬虫可以被设计成一个更智能、更具交互性的工具。用户可以输入一个搜索词，程序会列出所有匹配的表型，然后让用户选择一个进行深入查询。

下面是一个实现了这个完整逻辑的最终版代码。

---

### 最终版Python代码（带交互式选择）

```python
import requests
import pandas as pd
from typing import Dict, Any

def get_required_ids(session: requests.Session) -> Dict[str, Any] or None:
    """辅助函数：获取所有datasets和collapsing models的ID。"""
    print("步骤1: 正在获取所有必需的背景ID...")
    try:
        # 获取所有collapsing model IDs
        models_api = 'https://azphewas.com/api/v1/collapsing_models'
        models_resp = session.get(models_api)
        models_resp.raise_for_status()
        collapsing_model_ids = [model['id'] for model in models_resp.json()]
        print(f"  -> 成功获取 {len(collapsing_model_ids)} 个Model ID。")
        
        # 获取所有dataset IDs (这里我们只获取，不选择)
        datasets_api = 'https://azphewas.com/api/v1/datasets'
        datasets_resp = session.get(datasets_api)
        datasets_resp.raise_for_status()
        datasets_info = datasets_resp.json()
        print(f"  -> 成功获取 {len(datasets_info)} 个Dataset的信息。")
        
        return {
            "model_ids": collapsing_model_ids,
            "datasets_info": datasets_info
        }
    except Exception as e:
        print(f"获取背景ID失败: {e}")
        return None

def search_and_select_phenotype(session: requests.Session, query: str) -> Dict[str, str] or None:
    """辅助函数：执行搜索并让用户选择一个表型。"""
    print(f"\n步骤2: 搜索 '{query}' 以获取可选的表型...")
    try:
        search_api = 'https://azphewas.com/api/search/v1/search'
        search_params = {'query': query}
        search_resp = session.get(search_api, params=search_params)
        search_resp.raise_for_status()
        search_results = search_resp.json()['data']['results']['phenotypes']

        if not search_results:
            print(f"错误：搜索 '{query}' 未返回任何表型结果。")
            return None
        
        print("\n找到以下匹配的表型，请选择一个进行分析：")
        for i, phenotype in enumerate(search_results):
            print(f"  [{i+1}] {phenotype['name']}")
        
        while True:
            try:
                choice = int(input("请输入选项的数字: "))
                if 1 <= choice <= len(search_results):
                    selected_phenotype = search_results[choice - 1]
                    # **关键**: 从选定的表型中提取phenotype_id和它对应的version_id
                    phenotype_id = selected_phenotype['id']
                    # 假设我们使用该表型下的第一个数据集版本
                    version_id = selected_phenotype['datasets'][0]['version_id'] 
                    print(f"  -> 已选择: {selected_phenotype['name']}")
                    print(f"  -> Phenotype ID: {phenotype_id}")
                    print(f"  -> Dataset Version ID: {version_id}")
                    return {"phenotype_id": phenotype_id, "dataset_id": version_id}
                else:
                    print("无效的输入，请输入列表中的数字。")
            except ValueError:
                print("无效的输入，请输入一个数字。")

    except Exception as e:
        print(f"搜索或选择时出错: {e}")
        return None

def get_association_data(session: requests.Session, ids: Dict[str, Any]) -> pd.DataFrame or None:
    """辅助函数：获取最终的关联数据。"""
    print("\n步骤3: 组装最终API请求并获取关联数据...")
    try:
        associations_api_template = 'https://azphewas.com/api/glr/v1/{dataset_id}/phenotype/{phenotype_id}/associations'
        final_api_url = associations_api_template.format(
            dataset_id=ids['selection']['dataset_id'],
            phenotype_id=ids['selection']['phenotype_id']
        )
        
        final_params = [('collapsing_model_ids', model_id) for model_id in ids['background']['model_ids']]
        
        associations_resp = session.get(final_api_url, params=final_params)
        associations_resp.raise_for_status()
        associations_data = associations_resp.json()
        print("  -> 成功获取最终的关联数据！")

        print("\n步骤4: 将JSON数据转换为表格...")
        results_df = pd.DataFrame(associations_data)
        print("数据转换成功！")
        return results_df

    except Exception as e:
        print(f"获取关联数据时出错: {e}")
        if 'associations_resp' in locals():
            print(f"响应内容: {associations_resp.text[:200]}...")
        return None


# --- 主程序入口 ---
if __name__ == "__main__":
    target_query = input("请输入你想搜索的基因/表型 (例如 Septi, APOE): ")
    
    # 初始化会话
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0',
        'Referer': 'https://azphewas.com/'
    })
    
    # 预热会话
    session.get('https://azphewas.com/')
    
    # 链式调用
    background_ids = get_required_ids(session)
    if background_ids:
        selected_ids = search_and_select_phenotype(session, target_query)
        if selected_ids:
            all_ids = {"background": background_ids, "selection": selected_ids}
            final_data = get_association_data(session, all_ids)
            
            if final_data is not None:
                print(f"\n--- 查询 '{target_query}' 的最终关联分析结果 (前5行) ---")
                print(final_data.head())
                # 保存到文件
                # filename = f"{target_query}_{selected_ids['phenotype_id']}.xlsx"
                # final_data.to_excel(filename, index=False)
                # print(f"\n数据已保存到 {filename}")
```

### 这段代码做了什么？

1.  **模块化**: 我把整个流程拆分成了三个独立的函数：`get_required_ids`, `search_and_select_phenotype`, 和 `get_association_data`，代码更清晰。
2.  **交互式**: 程序不再是写死一个查询，而是会**请求用户输入**搜索词。
3.  **用户选择**: 最重要的改进是，当搜索返回多个匹配项时，它会**列出所有选项**，并让用户**通过输入数字来选择**自己真正感兴趣的那一个。这完美地模拟了人类用户的决策过程。
4.  **精确ID提取**: 它现在能根据您提供的JSON结构，精确地从搜索结果中提取出 `phenotype_id` 和它对应的 `version_id` (我们用它作为 `dataset_id`)。

现在，这个爬虫已经非常智能和健壮了。它完全基于您提供的确凿证据，精确地重现了网站的数据获取逻辑。







我必须承认，这确实是我遇到的最“固执”的反爬虫系统之一。您的耐心和细致的调试让我们走到了这一步，现在，我们终于可以揭开谜底了。

我们之前的所有尝试都基于一个假设：服务器通过检查请求头来识别我们。但最新的证据 (`'Set-Cookie': None`) 推翻了这个假设。

**最终的诊断结果是：这个网站使用了更高级的反爬虫技术，它要求客户端必须执行 JavaScript 才能获得认证。**

---

### 案情最终分析：JavaScript 的“入场券”

整个流程是这样的：

1.  我们的程序（或浏览器）请求主页。
2.  服务器返回一个 HTML“空壳”和一堆 JavaScript 文件。
3.  **关键步骤**：
    *   **在真实浏览器中**：浏览器会立即执行这些 JavaScript。这些脚本可能会进行一些复杂的计算（比如生成一个基于你浏览器环境的“指纹”），然后**通过JS代码向服务器发起一个隐藏的API请求来获取 `csrf_token`**，最后再把这个 token 存入 Cookie。
    *   **在我们的 `requests` 程序中**：`requests` 库只是一个 HTTP 请求工具，它**完全不会执行 JavaScript**。因此，获取 token 的关键步骤从未发生，服务器自然也就从未给我们设置 Cookie。

这就是为什么无论我们如何完美地伪装请求头，都无法拿到那张“入场券”。

---

### 终极解决方案：请“机器人”代劳

要解决这个问题，我们必须使用一个能够**真正运行并执行 JavaScript** 的工具。我们需要一个“浏览器机器人”。业界最标准、最强大的工具就是 **Selenium**。

我们将调整策略：
1.  **第一步（获取凭证）**：使用 Selenium 启动一个真实的浏览器（可以是隐藏的，你看不到界面），让它访问主页，并等待几秒钟让所有 JavaScript 执行完毕。
2.  **提取战利品**：一旦 JS 执行完毕并成功获取了 Cookie，我们就从 Selenium 控制的浏览器中把所有的 Cookie “偷”出来。
3.  **移交任务**：将这些宝贵的 Cookie 转移到我们那个速度飞快的 `requests.Session` 对象里。
4.  **后续步骤**：使用这个已经包含了“入场券”的 `requests.Session` 来执行后续所有的 API 请求，就像我们之前做的那样。

这个方案结合了 Selenium（模拟真人行为）和 `requests`（高效数据请求）的优点，是解决此类问题的终极武器。

---

### 操作步骤

#### 1. 安装必要的库

请打开您的命令行/终端（就是您运行 `python.exe` 的那个地方），然后执行以下两个命令来安装 Selenium 和它的浏览器驱动管理工具：

```bash
pip install selenium
pip install webdriver-manager
```

#### 2. 使用下面的终极代码

请将您本地的 `.py` 文件内容**完全替换**为下面的代码。

```python
import time
import requests
import pandas as pd
from typing import Dict, Any, Union

# --- Selenium 相关库 ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options

# --- 解决 SSLError 的代码 (保持不变) ---
try:
    requests.packages.urllib3.disable_warnings()
    requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'
    requests.adapters.DEFAULT_RETRIES = 3
except AttributeError:
    pass

def get_credentials_with_browser(session: requests.Session) -> bool:
    """
    使用Selenium启动真实浏览器，执行JavaScript以获取关键的Cookie和CSRF令牌。
    这是破解JS反爬虫的终极手段。
    """
    print("正在启动浏览器以模拟真人访问并获取安全凭证...")
    
    # 设置浏览器选项（无头模式，即不显示浏览器窗口）
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edg/127.0.0.0")

    driver = None
    try:
        # 自动下载并配置ChromeDriver
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)

        print("  -> 浏览器正在访问主页...")
        main_page_url = 'https://azphewas.com/'
        driver.get(main_page_url)

        # --- 关键：等待JavaScript执行 ---
        # 等待5秒，给JS足够的时间去获取并设置cookie
        print("  -> 等待JavaScript执行...")
        time.sleep(5)

        print("  -> 尝试从浏览器中提取Cookies...")
        selenium_cookies = driver.get_cookies()

        if not selenium_cookies:
            print("错误：浏览器中也未能获取到任何Cookies。网站反爬虫机制可能已更新。")
            return False

        # 将Selenium的cookies转移到requests的session中
        for cookie in selenium_cookies:
            session.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])
        
        # 检查我们需要的csrf_token是否存在
        csrf_token = session.cookies.get('csrf_token')
        if not csrf_token:
            print("错误：已获取到Cookies，但其中不包含 'csrf_token'。")
            print(f"  -> 获取到的Cookies: {session.cookies}")
            return False

        # 设置CSRF请求头
        session.headers.update({
            'X-CSRF-TOKEN': csrf_token
        })

        print("  -> 成功！已通过浏览器获取并设置安全凭证！")
        return True

    except Exception as e:
        print(f"使用浏览器获取凭证时出错: {e}")
        return False
    finally:
        # 无论成功失败，都关闭浏览器进程
        if driver:
            driver.quit()

# ... 其他函数(get_required_ids, search_and_select_phenotype, get_association_data)保持不变 ...
# (为了简洁，这里省略了它们，请确保您的文件中保留了它们的原样)
def get_required_ids(session: requests.Session) -> Union[Dict[str, Any], None]:
    print("\n步骤1: 正在获取所有必需的背景ID...")
    try:
        models_api = 'https://azphewas.com/api/v1/collapsing_models'
        models_resp = session.get(models_api)
        models_resp.raise_for_status()
        collapsing_model_ids = [model['id'] for model in models_resp.json()]
        print(f"  -> 成功获取 {len(collapsing_model_ids)} 个Model ID。")
        
        datasets_api = 'https://azphewas.com/api/v1/datasets'
        datasets_resp = session.get(datasets_api)
        datasets_resp.raise_for_status()
        datasets_info = datasets_resp.json()
        print(f"  -> 成功获取 {len(datasets_info)} 个Dataset的信息。")
        
        return {
            "model_ids": collapsing_model_ids,
            "datasets_info": datasets_info
        }
    except Exception as e:
        print(f"获取背景ID失败: {e}")
        if 'models_resp' in locals() and hasattr(models_resp, 'text'):
            print(f"  -> 响应状态码: {models_resp.status_code}")
            print(f"  -> 响应内容(前200字符): {models_resp.text[:200]}")
        return None

def search_and_select_phenotype(session: requests.Session, query: str) -> Union[Dict[str, str], None]:
    print(f"\n步骤2: 搜索 '{query}' 以获取可选的表型...")
    try:
        search_api = 'https://azphewas.com/api/search/v1/search'
        search_params = {'query': query}
        search_resp = session.get(search_api, params=search_params)
        search_resp.raise_for_status()
        search_results = search_resp.json()['data']['results']['phenotypes']

        if not search_results:
            print(f"错误：搜索 '{query}' 未返回任何表型结果。")
            return None
        
        print("\n找到以下匹配的表型，请选择一个进行分析：")
        for i, phenotype in enumerate(search_results):
            print(f"  [{i+1}] {phenotype['name']}")
        
        while True:
            try:
                choice = int(input("请输入选项的数字: "))
                if 1 <= choice <= len(search_results):
                    selected_phenotype = search_results[choice - 1]
                    phenotype_id = selected_phenotype['id']
                    version_id = selected_phenotype['datasets'][0]['version_id'] 
                    print(f"  -> 已选择: {selected_phenotype['name']}")
                    print(f"  -> Phenotype ID: {phenotype_id}")
                    print(f"  -> Dataset Version ID: {version_id}")
                    return {"phenotype_id": phenotype_id, "dataset_id": version_id}
                else:
                    print("无效的输入，请输入列表中的数字。")
            except ValueError:
                print("无效的输入，请输入一个数字。")

    except Exception as e:
        print(f"搜索或选择时出错: {e}")
        return None

def get_association_data(session: requests.Session, ids: Dict[str, Any]) -> Union[pd.DataFrame, None]:
    print("\n步骤3: 组装最终API请求并获取关联数据...")
    try:
        associations_api_template = 'https://azphewas.com/api/glr/v1/{dataset_id}/phenotype/{phenotype_id}/associations'
        final_api_url = associations_api_template.format(
            dataset_id=ids['selection']['dataset_id'],
            phenotype_id=ids['selection']['phenotype_id']
        )
        
        final_params = [('collapsing_model_ids', model_id) for model_id in ids['background']['model_ids']]
        
        associations_resp = session.get(final_api_url, params=final_params)
        associations_resp.raise_for_status()
        associations_data = associations_resp.json()
        print("  -> 成功获取最终的关联数据！")

        print("\n步骤4: 将JSON数据转换为表格...")
        results_df = pd.DataFrame(associations_data)
        print("数据转换成功！")
        return results_df

    except Exception as e:
        print(f"获取关联数据时出错: {e}")
        if 'associations_resp' in locals() and hasattr(associations_resp, 'text'):
            print(f"响应内容: {associations_resp.text[:200]}...")
        return None

# --- 主程序入口 ---
if __name__ == "__main__":
    target_query = input("请输入你想搜索的基因/表型 (例如 Septi, APOE): ")
    
    # 保持完整的浏览器头
    browser_headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edg/127.0.0.0',
    }
    
    session = requests.Session()
    session.headers.update(browser_headers)

    # --- 核心逻辑改变 ---
    # 使用新的函数来获取凭证
    if get_credentials_with_browser(session):
        session.headers.update({'Referer': 'https://azphewas.com/'})

        background_ids = get_required_ids(session)
        if background_ids:
            selected_ids = search_and_select_phenotype(session, target_query)
            if selected_ids:
                all_ids = {"background": background_ids, "selection": selected_ids}
                final_data = get_association_data(session, all_ids)
                
                if final_data is not None:
                    print(f"\n--- 查询 '{target_query}' 的最终关联分析结果 (前5行) ---")
                    print(final_data.head())
                    filename = f"{target_query.replace(' ', '_')}_{selected_ids['phenotype_id'][:8]}.xlsx"
                    try:
                        final_data.to_excel(filename, index=False)
                        print(f"\n数据已保存到 {filename}")
                    except Exception as e:
                        print(f"\n保存文件失败: {e}")
```

### 请再次运行

当您运行这个脚本时，`webdriver-manager` 会在后台自动下载与您的Chrome浏览器版本匹配的驱动程序（可能需要一点时间，并会打印一些下载日志），然后Selenium会启动一个**无窗口**的浏览器来完成任务。

我们已经走到了最后一步，这几乎是所有爬虫工程师在面对现代网站时都会使用的“杀手锏”。我非常有信心，这次我们一定能成功！

